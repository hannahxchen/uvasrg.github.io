<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>publications | Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      
<div style="margin-top:16px; margin-left: auto; margin-right: auto; max-width: 800px;">
<div class="row">
  <div class="column small-12">
    
    
    <h2><a href="/chinese-translation-of-mpc-book/">Chinese Translation of MPC Book</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-06-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 June 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/secure-computation">secure computation</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/vladimir-kolesnikov">Vladimir Kolesnikov</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mike-rosulek">Mike Rosulek</a>
    
  </span>
  
  
</div>


<a href="https://item.jd.com/13302742.html">
<img src="https://securecomputation.org/images/chinese-cover.png" width=25% align="right" style="margin-bottom: 12px; margin-left: 12px; margin-right: 20px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);"></a>
<p>A Chinese translation of our <a href="//securecomputation.org"><em>A Pragmatic Introduction to Secure
Multi-Party Computation</em></a> book (by David
Evans, Vladimir Kolesnikov, and Mike Rosulek) is now available!</p>
<p>Thanks to Weiran Liu and Sengchao Ding for all the work they
did on the translation.</p>
<p>To order from JD.com: <a href="https://item.jd.com/13302742.html"><em>https://item.jd.com/13302742.html</em></a></p>
<p>(The English version of the book is still available for free download, from <a href="https://securecomputation.org"><em>https://securecomputation.org</em></a>.)</p>

	

    
    <h2><a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-04-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 April 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jack-prescott">Jack Prescott</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>


Our paper on Improved Estimation of Concentration Under ℓp-Norm Distance Metrics Using Half Spaces (Jack Prescott, Xiao Zhang, and David Evans) will be presented at ICLR 2021.
Abstract: Concentration of measure has been argued to be the fundamental cause of adversarial vulnerability. Mahloujifar et al. (2019) presented an empirical way to measure the concentration of a data distribution using samples, and employed it to find lower bounds on intrinsic robustness for several benchmark datasets.
<p class="text-right"><a href="/improved-estimation-of-concentration-iclr-2021/">Read More…</a></p>
	

    
    <h2><a href="/algorithmic-accountability-and-the-law/">Algorithmic Accountability and the Law</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-12-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 December 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fairness">fairness</a>
    
  </span>
  
  
</div>


Brink News (a publication of The Atlantic) published an essay I co-authored with Tom Nachbar (UVA Law School) on how the law views algorithmic accountability and the limits of what measures are permitted under the law to adjust algorithms to counter inequity:
 Algorithms Are Running Foul of Anti-Discrimination Law
Tom Nachbar and David Evans
Brink, 7 December 2020  

Computing systems that are found to discriminate on prohibited bases, such as race or sex, are no longer surprising.
<p class="text-right"><a href="/algorithmic-accountability-and-the-law/">Read More…</a></p>
	

    
    <h2><a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Merlin, Morgan, and the Importance of Thresholds and Priors</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-10-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 October 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/lingxiao-wang">Lingxiao Wang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/quanquan-gu">Quanquan Gu</a>
    
  </span>
  
  
</div>


Post by Katherine Knipmeyer
Machine learning poses a substantial risk that adversaries will be able to discover information that the model does not intend to reveal. One set of methods by which consumers can learn this sensitive information, known broadly as membership inference attacks, predicts whether or not a query record belongs to the training set. A basic membership inference attack involves an attacker with a given record and black-box access to a model who tries to determine whether said record was a member of the model’s training set.
<p class="text-right"><a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Read More…</a></p>
	

    
    <h2><a href="/robustrepresentations/">Adversarially Robust Representations</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/icml">ICML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sicheng-zhu">Sicheng Zhu</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>
    
  </span>
  
  
</div>


Post by Sicheng Zhu
With the rapid development of deep learning and the explosive growth of unlabeled data, representation learning is becoming increasingly important. It has made impressive applications such as pre-trained language models (e.g., BERT and GPT-3).
Popular as it is, representation learning raises concerns about the robustness of learned representations under adversarial settings. For example, how can we compare the robustness to different representations, and how can we build representations that enable robust downstream classifiers?
<p class="text-right"><a href="/robustrepresentations/">Read More…</a></p>
	

    
    <h2><a href="/intrinsic-robustness-using-conditional-gans/">Intrinsic Robustness using Conditional GANs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/aistats">AISTATS</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jinghui-chen">Jinghui Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/quanquan-gu">Quanquan Gu</a>
    
  </span>
  
  
</div>


The video of Xiao&rsquo;s presentation for AISTATS 2020 is now available: Understanding the Intrinsic Robustness of Image Distributions using Conditional Generative Models
Starting with Gilmer et al. (2018), several works have demonstrated the inevitability of adversarial examples based on different assumptions about the underlying input probability space. It remains unclear, however, whether these results apply to natural image distributions. In this work, we assume the underlying data distribution is captured by some conditional generative model, and prove intrinsic robustness bounds for a general class of classifiers, which solves an open problem in Fawzi et al.
<p class="text-right"><a href="/intrinsic-robustness-using-conditional-gans/">Read More…</a></p>
	

    
    <h2><a href="/hybrid-batch-attacks-at-usenix-security-2020/">Hybrid Batch Attacks at USENIX Security 2020</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-08-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 August 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jianfeng-chi">Jianfeng Chi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/usenix-security">USENIX Security</a>
    
  </span>
  
  
</div>


<p>Here&rsquo;s the video for Fnu Suya&rsquo;s presentation on <a href="/usenix-security-2020-hybrid-batch-attacks">Hybrid Batch Attacks</a> at USENIX Security 2020:</p>
<center>
  <video width="90%" id="usenix-media-video-1" data-setup="{}" poster="" class="video-js vjs-default-skin vjs-big-play-centered" preload="auto" controls>
    <source src='https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'>
  </video><br> 
<a href="https://2459d6dc103cb5933875-c0245c5c937c5dedcca3f1764ecc9b2f.ssl.cf2.rackcdn.com/sec20/videos/0813/s5_machine_learning_1/3_sec20summer-paper412-presentation-video.mp4">Download Video [mp4]</a></p>
</center>
<p><a href="/usenix-security-2020-hybrid-batch-attacks">Blog Post</a><br>
Paper: [<a href="/docs/hybrid_attack.pdf">PDF</a>] [<a href="https://arxiv.org/abs/1908.07000">arXiv</a>]</p>

	

    
    <h2><a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">Pointwise Paraphrase Appraisal is Potentially Problematic</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-07-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 July 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/natural-language-processing">natural language processing</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannah-chen">Hannah Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yangfeng-ji">Yangfeng Ji</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/acl">ACL</a>
    
  </span>
  
  
</div>


Hannah Chen presented her paper on Pointwise Paraphrase Appraisal is Potentially Problematic at the ACL 2020 Student Research Workshop:
 The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models.
<p class="text-right"><a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">Read More…</a></p>
	

    
    <h2><a href="/usenix-security-2020-hybrid-batch-attacks/">USENIX Security 2020: Hybrid Batch Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-12-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 December 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jianfeng-chi">Jianfeng Chi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/usenix-security">USENIX Security</a>
    
  </span>
  
  
</div>


New: Video Presentation
Finding Black-box Adversarial Examples with Limited Queries Black-box attacks generate adversarial examples (AEs) against deep neural networks with only API access to the victim model.
Existing black-box attacks can be grouped into two main categories:
  Transfer Attacks use white-box attacks on local models to find candidate adversarial examples that transfer to the target model.
  Optimization Attacks use queries to the target model and apply optimization techniques to search for adversarial examples.
<p class="text-right"><a href="/usenix-security-2020-hybrid-batch-attacks/">Read More…</a></p>
	

    
    <h2><a href="/neurips-2019-empirically-measuring-concentration/">NeurIPS 2019: Empirically Measuring Concentration</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2019-11-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 November 2019</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/neurips">NeurIPS</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/saeed-mahloujifar">Saeed Mahloujifar</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/mohammad-mahmoody">Mohammad Mahmoody</a>
    
  </span>
  
  
</div>


Xiao Zhang will present our work (with Saeed Mahloujifar and Mohamood Mahmoody) as a spotlight at NeurIPS 2019, Vancouver, 10 December 2019.
Recent theoretical results, starting with Gilmer et al.&lsquo;s Adversarial Spheres (2018), show that if inputs are drawn from a concentrated metric probability space, then adversarial examples with small perturbation are inevitable.c The key insight from this line of research is that concentration of measure gives lower bound on adversarial risk for a large collection of classifiers (e.
<p class="text-right"><a href="/neurips-2019-empirically-measuring-concentration/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/categories/publications/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 2</span></li>      
      
    </ul>    
    All Posts by <a href="//uvasrg.github.io/categories">Category</a> or <a href="//uvasrg.github.io/tags">Tags</a>.

  </div>
</div>

  </div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-6 medium-3">
      <img src="/images/uva_primary_rgb.png">
      </div>
    <div class="column small-6 medium-3">
      <a href="/"><b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
      <a href="mailto:evans@virginia.edu"><em>evans@virginia.edu</em></a>
    </div>
    <div classs="column small-4 medium-2"></div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
