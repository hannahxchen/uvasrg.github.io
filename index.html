<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.71.0" />
    <meta charset="utf-8">
    <title>Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





<div class="container">
 <div>

    <div class="column small-18 medium-9">
      
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="centersecurity-and-privacy-research-at-the-university-of-virginiacenter"><center>Security and Privacy Research at the University of Virginia</center></h1>
<p></p>
<div class="row">
<div class="column small-25 medium-7">
<p>Our research seeks to empower individuals and organizations to control
how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the privacy and security of
computing as practiced today, and as envisioned in the future. A major
current focus is on <em>adversarial machine learning</em>.</p>
</p> 
 <p>
<p>Everyone is welcome at our research group meetings. To get
announcements, join our <a
href="https://uvasrg.slack.com/signup">Slack Group</a> (any
<em>@virginia.edu</em> email address can join themsleves, or email me
to request an invitation). </p> </div> <div class="column small-5
medium-5"> <center> <a
href="/images/srg2017/IMG_20171212_135015.jpg"><img
src="/images/srg2017/IMG_20171212_135015-2.jpg" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font
size="-1">(12 December 2017)</font><br> <div
class="smallcaption"> <a
href="https://hainali.github.io/">Haina Li</a>, Felix Park,
<a
href="https://sites.google.com/site/mahmadjonas/">Mainuddin Jonas</a>,
<A
href="https://www.linkedin.com/in/anant-kharkar-502433b9">Anant Kharkar</a>,
<a
href="http://dblp2.uni-trier.de/pers/hd/s/Shezan:Faysal_Hossain">Faysal Hossain Shezan</a>,
<A href="https://fsuya.org/">Fnu Suya</a>, <A
href="https://www.cs.virginia.edu/evans">David Evans</a>, <a
href="https://www.yuantiancmu.com/">Yuan Tian</a>, <a
href="//www.cs.columbia.edu/~riley/">Riley Spahn</a>, <a
href="//www.cs.virginia.edu/~wx4ed/">Weilin Xu</a>, <a
href="https://github.com/gjverrier">Guy &ldquo;Jack&rdquo; Verrier</a>
</font> </center> </p> </div> </div></p>
<div class="row">
<div class="column small-10 medium-5">
<div class="mainsection">Active Projects</div>
<p><a href="/privacy/"><b>Privacy for Machine Learning</b></a> <br>
<a href="//www.evademl.org/"><b>Security for Machine Learning</b> (EvadeML)</a><br>
<a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">NLP Robustness</a></p>
</div>
<div class="column small-14 medium-7">
<div class="mainsection">Other Projects</div>
<em>
<a href="//securecomputation.org">Secure Multi-Party Computation</a></em>:
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a><br>
<p><em>Web and Mobile Security</em>: <a href="http://www.scriptinspector.org/">ScriptInspector</a> ·
<a href="http://www.ssoscan.org/">SSOScan</a><br>
<em>Program Analysis</em>: <a href="//www.splint.org/">Splint</a> · <a href="//www.cs.virginia.edu/perracotta">Perracotta</a><br>
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> ·
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> ·
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a></p>
</p>
</div>
</div>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/codaspy-2021-keynote-when-models-learn-too-much/">Codaspy 2021 Keynote: When Models Learn Too Much</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-04-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">26 April 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Here are the slides for my talk at the
<a href="http://www.codaspy.org/2021/program.html">11th ACM Conference on Data and Application Security and Privacy</a>:</p>
<center>
[**When Models Learn Too Much** [PDF]](https://www.dropbox.com/s/6wzloxuai709s0k/codaspy-post.pdf?dl=0)
</center>
<p>The talk includes Bargav Jayaraman&rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy in machine learning (as well as more recent work by Anshuman Suri on property inference attacks, and Bargav on attribute inference and imputation):</p>
<ul>
<li><a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/"><em>Merlin, Morgan, and the Importance of Thresholds and Priors</em></a></li>
<li><a href="/evaluating-differentially-private-machine-learning-in-practice/"><em>Evaluating Differentially Private Machine Learning in Practice</em></a></li>
</ul>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">“When models learn too much. “ Dr. David Evans <a href="https://twitter.com/UdacityDave?ref_src=twsrc%5Etfw">@UdacityDave</a> of University of Virginia gave a keynote talk on different inference risks for machine learning models this morning at <a href="https://twitter.com/hashtag/codaspy21?src=hash&amp;ref_src=twsrc%5Etfw">#codaspy21</a> <a href="https://t.co/KVgFoUA6sa">pic.twitter.com/KVgFoUA6sa</a></p>&mdash; acmcodaspy (@acmcodaspy) <a href="https://twitter.com/acmcodaspy/status/1386748565796507652?ref_src=twsrc%5Etfw">April 26, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</center>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/crysp-talk-when-models-learn-too-much/">CrySP Talk: When Models Learn Too Much</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-04-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 April 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/differential-privacy">differential privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/videos">videos</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I gave a talk on <a href="https://crysp.uwaterloo.ca/speakers/20210329-Evans"><em>When Models Learn Too Much</em></a> at the University of Waterloo (virtually) in the CrySP
Speaker Series on Privacy (29 March 2021):</p>
<center>
<iframe width="900" height="315" src="https://www.youtube-nocookie.com/embed/LM_-N76_KIw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<center>
<b>Abstract</b>
</center>
<p>Statistical machine learning uses training data to produce models that
capture patterns in that data. When models are trained on private
data, such as medical records or personal emails, there is a risk that
those models not only learn the hoped-for patterns, but will also
learn and expose sensitive information about their training
data. Several different types of inference attacks on machine learning
models have been found, and methods have been proposed to mitigate the
risks of exposing sensitive aspects of training data. Differential
privacy provides formal guarantees bounding certain types of inference
risk, but, at least with state-of-the-art methods, providing
substantive differential privacy guarantees requires adding so much
noise to the training process for complex models that the resulting
models are useless. Experimental evidence, however, suggests that
inference attacks have limited power, and in many cases a very small
amount of privacy noise seems to be enough to defuse inference
attacks. In this talk, I will give an overview of a variety of
different inference risks for machine learning models, talk about
strategies for evaluating model inference risks, and report on some
experiments by our research group to better understand the power of
inference attacks in more realistic settings, and explore some broader
the connections between privacy, fairness, and adversarial robustness.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-04-02 00:00:00 &#43;0000 UTC" itemprop="datePublished">2 April 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jack-prescott">Jack Prescott</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Our paper on <a href="https://openreview.net/forum?id=BUlyHkzjgmA"><em>Improved Estimation of Concentration Under ℓ<sub>p</sub>-Norm Distance Metrics Using Half Spaces</em></a> (Jack Prescott, <a href="https://people.virginia.edu/~xz7bc/">Xiao Zhang</a>, and David Evans) will be presented at ICLR 2021.</p>
<p><b>Abstract:</b> Concentration of measure has been argued to be the
fundamental cause of adversarial vulnerability. Mahloujifar et
al. (2019) presented an empirical way to measure the concentration of
a data distribution using samples, and employed it to find lower
bounds on intrinsic robustness for several benchmark
datasets. However, it remains unclear whether these lower bounds are
tight enough to provide a useful approximation for the intrinsic
robustness of a dataset. To gain a deeper understanding of the
concentration of measure phenomenon, we first extend the Gaussian
Isoperimetric Inequality to non-spherical Gaussian measures and
arbitrary ℓ<sub>p</sub>-norms (<em>p</em> ≥ 2). We leverage these
theoretical insights to design a method that uses half-spaces to
estimate the concentration of any empirical dataset under
ℓ<sub>p</sub>-norm distance metrics. Our proposed algorithm is more
efficient than Mahloujifar et al. (2019)&lsquo;s, and experiments on
synthetic datasets and image benchmarks demonstrate that it is able to
find much tighter intrinsic robustness bounds. These tighter estimates
provide further evidence that rules out intrinsic dataset
concentration as a possible explanation for the adversarial
vulnerability of state-of-the-art classifiers.</p>
<p>Here&rsquo;s Jack&rsquo;s video summary of the work:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/VELmIHq09pQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<p>Paper: <a href="https://arxiv.org/abs/2103.12913">arXiv</a>, <a href="https://openreview.net/forum?id=BUlyHkzjgmA">Open Review</a><br>
Code: <a href="https://github.com/jackbprescott/EMC_HalfSpaces"><em>https://github.com/jackbprescott/EMC_HalfSpaces</em></a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/virginia-consumer-data-protection-act/">Virginia Consumer Data Protection Act</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-02-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 February 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/law">law</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="https://www.josephinelamp.com/">Josephine Lamp</a> presented on the new data privacy law that is pending in Virginia (it still needs a few steps including expected signing by governor, but likely to go into effect Jan 1, 2023): <a href="https://www.dropbox.com/s/1epulyhc30wd239/cdpa.pdf?dl=0">Slides (PDF)</a></p>
<p>This article provides a summary of the law: <a href="https://www.natlawreview.com/article/virginia-passes-consumer-privacy-law-other-states-may-follow"><em>Virginia Passes Consumer Privacy Law; Other States May Follow</em></a>, National Law Review, 17 February 2021.</p>
<p>The law itself is here: <a href="https://lis.virginia.gov/cgi-bin/legp604.exe?211+ful+SB1392">SB 1392: Consumer Data Protection Act</a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/algorithmic-accountability-and-the-law/">Algorithmic Accountability and the Law</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2020-12-14 00:00:00 &#43;0000 UTC" itemprop="datePublished">14 December 2020</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fairness">fairness</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Brink News (a publication of <em>The Atlantic</em>) published an essay I
co-authored with Tom Nachbar (UVA Law School) on how the law views
algorithmic accountability and the limits of what measures are
permitted under the law to adjust algorithms to counter inequity:</p>
<center>
<a
href="https://www.brinknews.com/algorithms-are-running-foul-of-anti-discrimination-law/"><em><b>Algorithms
Are Running Foul of Anti-Discrimination Law</em></b></a><br>
Tom Nachbar and David Evans<br>
Brink, 7 December 2020
</center>
<p><br></br></p>
<p>Computing systems that are found to discriminate on prohibited bases, such as race or sex, are no longer surprising. We’ve seen hiring systems that discriminate <a href="https://www.brinknews.com/ethics-codes-are-not-enough-to-curb-the-danger-of-bias-in-ai/">against women</a> image systems that are prone to <a href="https://www.theatlantic.com/family/archive/2020/10/algorithmic-bias-especially-dangerous-teens/616793/">cropping out dark-colored faces</a> and <a href="https://www.theatlantic.com/technology/archive/2016/12/how-algorithms-can-bring-down-minorities-credit-scores/509333/">credit scoring systems that discriminate against minorities</a>.</p>
<p>Anyone considering deploying an algorithm that impacts humans needs to understand the potential for such algorithms to discriminate. But what to do about it is much less clear.</p>
<h2 id="the-difficulty-of-mandating-fairness">The Difficulty of Mandating Fairness</h2>
<p>There are no simple ways to ensure that an algorithm doesn’t
discriminate, and many of the proposed fixes run the risk of violating
anti-discrimination law. In particular, approaches that seek to
optimize computing systems for various notions of fairness, especially
those concerned with the distribution of outcomes along legally
protected criteria such as race or sex, are in considerable tension
with U.S. anti-discrimination law.</p>
<p>Although many arguments about discriminatory algorithms are premised
on unfair outcomes, such notions have limited relevance under
U.S. law.</p>
<p>For the most part, </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3530053">U.S. law</a> lacks a notion of fairness.</p>
<p>Legal rules generally call upon more specific notions than fairness,
even if they are connected to fairness. Thus, in the context of
illegal employment discrimination (which we will use as our motivating
example), instead of mandating fairness, U.S. law generally prohibits
conduct that discriminates on the basis of protected characteristics,
like race and sex.</p>
<h2 id="process-and-intent-matter">Process and Intent Matter</h2>
<p>Moreover, the law does not
generally regulate behavior based on outcomes; what matters is the
intent and process that led to the outcome. In the case of
U.S. employment discrimination law, those rules of intent and process
are contained in two types of protections against discrimination:
<em>disparate treatment</em> and
<em>disparate impact</em>.</p>
<p>An employer is liable for disparate treatment when there is either
explicit or intentional discrimination. Disparate treatment
protections prohibit the use of overt racial classifications, but also
provide liability for hidden but intentional discrimination, such as
cases where a victim can show they are a member of a racial minority
and were qualified, but were rejected, and the employer cannot provide
any nondiscriminatory justification for the decision, such as the case
of <a
href="https://supreme.justia.com/cases/federal/us/411/792/">McDonnell Douglas Corp. v. Green</a>.</p>
<p>Under disparate impact, an employer following a process that has a
statistically observable negative impact on a protected group is not
necessarily liable. Instead, the disparate outcomes transfer the
burden to the employer to show that the decision-making process is
justified based on valid criteria, as with the case of <a
href="https://supreme.justia.com/cases/federal/us/401/424/"><em>Griggs v. Duke Power Co.</em></a>.</p>
<p>If you think those two approaches sound confusingly similar, you’re not alone. Disparate impact liability <a
href="https://scholarship.law.gwu.edu/cgi/viewcontent.cgi?article=2030">frequently mirrors</a> disparate treatment liability in that the disparate outcome itself is
not enough to establish a violation.</p>
<p>The role of disparate outcomes is to shift the burden to the employer
to provide a non-discriminatory reason for its decision-making
process. What matters legally is not so much the outcome, as the
<em>intention</em> and <em>process</em> behind it.</p>
<p>Correcting for past racial disparities will require a more
sophisticated and deep-seated approach than simply altering
algorithms.</p>
<h2 id="the-law-doesnt-care-whether-decisions-are-made-by-algorithms-or-humans">The Law Doesn’t Care Whether Decisions Are Made by Algorithms or Humans</h2>
<p>When outcomes are based on the output of some algorithm, the employer
still needs to justify that the decisions it makes are based on valid
criteria. The law doesn’t care whether decisions are made by
algorithms or by human decision-makers; what matters is the reason for
the decision. It is up to the humans responsible to explain that
reasoning.</p>
<p>Although many have argued for increased algorithmic transparency, even
the most transparent algorithms cannot really explain <em>why</em>
they made the decisions they did. This presents a major challenge for
discrimination law because, in discrimination law, the “why”
matters.</p>
<p>Algorithmically generated explanations can help but, by themselves,
cannot answer the legal “why” question. Even an interpretable model
that appears to have no discriminatory intent is not necessarily
non-discriminatory. The rules it has learned could have been
influenced by selecting training data that disadvantages a particular
group, and the features it uses could be determined in ways that are
inherently discriminatory.</p>
<p>To satisfy discrimination law, it is the process and the intent
that matter, and explanations an algorithm itself produces are
insufficient to establish that intent. Indeed, explanations of the
intent of algorithms should be viewed with the same skepticism that we
have when humans attempt to describe their own decision-making
processes. Just because an explanation is provided does not mean it
should be believed.</p>
<h2 id="optimizing-an-algorithm-for-fairness-may-be-discriminatory">Optimizing an Algorithm for Fairness May Be Discriminatory</h2>
<p>This is a particular problem for methods used by systems designers,
who frequently seek to optimize for particular outcomes. An
optimization approach does not fit well with legal requirements. When
algorithm designers focus on fairness as a property to be optimized,
they ignore the legal requirements of anti-discrimination.</p>
<p>Discrimination law does not operate through optimization, because
discrimination (or anti-discrimination) is not something to be
optimized. Anti-discrimination is a <em>side constraint</em> on a
decision-making process, not its principal goal (e.g., to find good
employees).</p>
<p>Systems should be designed to optimize for their principal goal, with
the constraint of avoiding discrimination (in intent or process) while
doing so. Attempts to produce outcomes that seem less discriminatory
might themselves constitute illegal discrimination. The 2009
U.S. Supreme Court case of <a
href="https://supreme.justia.com/cases/federal/us/557/557/"><em>Ricci
v. DeStafano</em></a> provides a prime example of that tension. In the
case, the New Haven Fire Department used an examination to determine
which firefighters should be promoted to lieutenant. When that test
produced a result that was racially skewed compared to the population
of firefighters, the city (in part because they were concerned about
disparate impact liability), invalidated the results of the
test. White firefighters sued, claiming the city’s response in
rejecting the test was itself disparate treatment, since the
motivation for rejecting the test was to produce a different racial
outcome, and the Supreme Court agreed.</p>
<h2 id="algorithms-alone-cannot-save-us">Algorithms Alone Cannot Save Us</h2>
<p>Although reducing racial disparity is a laudable goal, the law
substantially limits the discretion of both employers and system
designers in engineering for equitable outcomes. Racially disparate
outcomes may seem unfair, and they might even be evidence of
underlying illegal discrimination, but the law neither deputizes
systems designers to operationalize their notions of what are racially
fair outcomes, nor immunizes them for acts of discrimination
undertaken in order to correct racial disparities.</p>
<p>Correcting for past racial disparities will require a more
sophisticated and deep-seated approach than simply altering algorithms
to produce outcomes optimized toward some fairness
criterion.</p>
<p>Algorithms alone are neither the source nor the solution to our
problems. Solving them will require fundamental change, and the real
question is whether we as a society — not just our algorithms
— are prepared to do that work.</p>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>
   </div>
    </div>

    <div class="column small=7 medium-3">
    <div class="sidebar">
University of Virginia <br>
Security Research Group
   </p>
   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>
   <p>
   <a href="/team"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
   </p>
<p class="nogap">
     <p>
   <a href="https://uvasrg.slack.com/"><b>Join Slack Group</b></a>
   </p>

  <a href="/studygroups/"><b>Study Groups</b></a>
  <div class="posttitle">  
    <a href="/advml">Adversarial Machine Learning</a>
  </div>
  <div class="posttitle">
    <a href="/privacy">Privacy</a>
   </div></p>
   <p class="nogap">
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/codaspy-2021-keynote-when-models-learn-too-much/">Codaspy 2021 Keynote: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/crysp-talk-when-models-learn-too-much/">CrySP Talk: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a>


   </div>
   
   <div class="posttitle">
      <a href="/virginia-consumer-data-protection-act/">Virginia Consumer Data Protection Act</a>


   </div>
   
   <div class="posttitle">
      <a href="/algorithmic-accountability-and-the-law/">Algorithmic Accountability and the Law</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/">Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</a>


   </div>
   
   <div class="posttitle">
      <a href="/fact-checking-donald-trumps-tweet-firing-christopher-krebs/">Fact-checking Donald Trump’s tweet firing Christopher Krebs</a>


   </div>
   
   <div class="posttitle">
      <a href="/voting-security/">Voting Security</a>


   </div>
   
   <div class="posttitle">
      <a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Merlin, Morgan, and the Importance of Thresholds and Priors</a>


   </div>
   
   <div class="posttitle">
      <a href="/intrinsic-robustness-using-conditional-gans/">Intrinsic Robustness using Conditional GANs</a>


   </div>
   
   <div class="posttitle">
      <a href="/robustrepresentations/">Adversarially Robust Representations</a>


   </div>
   
   <div class="posttitle">
      <a href="/hybrid-batch-attacks-at-usenix-security-2020/">Hybrid Batch Attacks at USENIX Security 2020</a>


   </div>
   
   <div class="posttitle">
      <a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">Pointwise Paraphrase Appraisal is Potentially Problematic</a>


   </div>
   
   <div class="posttitle">
      <a href="/de-naming-the-blog/">De-Naming the Blog</a>


   </div>
   
   <div class="posttitle">
      <a href="/oakland-test-of-time-awards/">Oakland Test-of-Time Awards</a>


   </div>
   
   <div class="posttitle">
      <a href="/neurips2019/">NeurIPS 2019</a>


   </div>
   
   <div class="posttitle">
      <a href="/usenix-security-2020-hybrid-batch-attacks/">USENIX Security 2020: Hybrid Batch Attacks</a>


   </div>
   
   <div class="posttitle">
      <a href="/neurips-2019-empirically-measuring-concentration/">NeurIPS 2019: Empirically Measuring Concentration</a>


   </div>
   
   <div class="posttitle">
      <a href="/white-house-visit/">White House Visit</a>


   </div>
   
   <div class="posttitle">
      <a href="/jobs-for-humans-2029-2059/">Jobs for Humans, 2029-2059</a>


   </div>
   
   <div class="posttitle">
      <a href="/research-symposium-posters/">Research Symposium Posters</a>


   </div>
   
   <div class="posttitle">
      <a href="/cantors-no-longer-lost-proof/">Cantor&#39;s (No Longer) Lost Proof</a>


   </div>
   
   <div class="posttitle">
      <a href="/fosad2019/">FOSAD Trustworthy Machine Learning Mini-Course</a>


   </div>
   
   <div class="posttitle">
      <a href="/evaluating-differentially-private-machine-learning-in-practice/">Evaluating Differentially Private Machine Learning in Practice</a>


   </div>
   
   <div class="posttitle">
      <a href="/usenix-security-symposium-2019/">USENIX Security Symposium 2019</a>


   </div>
   
<p></p>
   <div class="posttitle"><a href="/post/">Older Posts</a></div>
  <div class="posttitle"><a href="/tags">Posts by Tag</a></div>
  <div class="posttitle"><a href="/categories/">Posts by Category</a></div>
  <div class="posttitle"><a href="2017.html">Old Blog</a></div>
<p></p>
<p>
<a href="/awards/"><b>Awards</b></a>
   </p>
<p>

  </p>

<p><br></br></p>

   <p>
     <center>
       <img src="/images/uva_primary_rgb_white.png" width="80%">
       </center>
</p>

    </div>
</div>

   </div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-6 medium-3">
      <img src="/images/uva_primary_rgb.png">
      </div>
    <div class="column small-6 medium-3">
      <a href="/"><b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
      <a href="mailto:evans@virginia.edu"><em>evans@virginia.edu</em></a>
    </div>
    <div classs="column small-4 medium-2"></div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
