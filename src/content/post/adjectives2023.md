+++
date = "07 Aug 2023"
draft = true
title = "Adjectives Can Reveal Gender Biases Within NLP Models"
author = "Jason Briegel and Hannah Chen"
categories = ["research"]
tags = ["Jason Briegel", "Hannah Chen", "adversarial machine learning", "LLMs", "generative AI", "bias", "machine learning"]
+++

# Adjectives Can Reveal Gender Biases Within NLP Models
(Post by Jason Briegel and Hannah Chen)

This work expands upon the methodology behind

[WinoBias](https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino)
is a benchmark that is a collection of sentences and questions
designed to evaluate gender bias in NLP models by revealing what a
model has learned about gender stereotypes associated with
occupations. The goal of this work is to extend the WinoBias dataset
by incorporating gender-associated adjectives.  We measure the level
of bias produced by GPT-3.5 model with and without the adjectives
describing the professions. We show that the addition of adjectives
enable more revealing measurements of the underlying biases in a model
and how they can result in incorrect predictions.

## Introduction

Because NLP models are trained with human corpora (and now,
increasingly on text generated by other NLP models that were
originally trained on human language), they are prone to inheriting
common human stereotypes and biases. This is problematic, because with
their growing prominence they may further propagate these stereotypes
[(Sun et al., 2019)](https://arxiv.org/abs/1906.08976). For example,
interest is growing in mitigating bias in the field of machine
translation, where previously systems such as Google translate default
to translating gender-neutral pronouns as male pronouns, even with
feminine cues [(Savoldi et al.,
2021)](https://doi.org/10.1162/tacl_a_00401). 

Previous work has developed new corpora to evaluate gender bias in
models based on gender stereotypes ([Zhao et al.,
2018](https://aclanthology.org/N18-2003/); [Rudinger et al.,
2018](https://aclanthology.org/N18-2002/); [Nadeem et al.,
2021](https://aclanthology.org/2021.acl-long.416/)). WinoBias dataset
is designed to test whether the model is more likely to associate
gender pronouns to their stereotypical occupations [(Zhao et al.,
2018)](https://aclanthology.org/N18-2003/). For example, if a model is
found to be more likely to associate secretary with female rather than
male pronouns, it would be considered biased.

Adjectives can also elicit very strong gender biases. Previous work has analyzed language surrounding how professors and celebrities were written about, and some adjectives were found to be more associated with certain gender subjects [(Chang and McKeown, 2019)](https://arxiv.org/abs/1909.00091).

Given the strong correlation between gender and adjectives, we hypothesize that introducing gender-associated adjectives to the existing gender bias dataset may increase the level of bias shown by the model. We think that the combination of gender-associated adjectives and stereotypically gendered occupations may increase the gender cue in the model and further increase the bias measured by the predefined metric. For example, the model may consider *"tough mechanic"* as more masculine than just *"mechanic"*.


## Method

### Background

WinoBias dataset consists 395 pairs of "pro-stereotyped" and "anti-stereotyped" English sentences [(Zhao et al., 2018)](https://aclanthology.org/N18-2003/). Each sentence includes two occupations, one stereotypically male and one stereotypically female, as well as a pronoun or pronouns referring to one of the two occupations. The dataset is designed as a coreference resolution task in which the goal of the model is to correctly identify which occupation the pronoun refers to in the sentence. "Pro-stereotyped" sentences contain stereotypical association between gender and occupations, whereas "anti-stereotyped" sentences require linking gender to anti-stereotypical occupations. The two sentences in each pair are mostly identical except that the gendered pronouns are swapped.

<!-- The dataset contains two types of templates for producing the pairs. For type 1 sentences, the decisions can only be made with world knowledge since they contain no syntactic cues. The models are expected to do well on type 2 sentences as they contain both syntactic and semantic cues. In this work, we will only focus on the type 1 sentences.  -->

The model is considered biased if the model performs better on the pro-stereotyped than the anti-stereotyped sentences. On the other hand, the model is unbiased if the model performs equally well on both pro-stereotyped and anti-stereotyped sentences. This methodology is useful for auditing bias, but the actual corpus itself was somewhat limited, as noted by the authors. In particular, it only detects bias regarding professions.

### Inserting Adjectives

We expand upon the original WinoBias corpus by inserting gender-associated adjectives describing the two professions. We consider two ways of inserting the adjectives: (1) inserting a contrasting pair of adjectives to both of the occupations in the sentence and (2) inserting only one adjective to one of the occupations. The contrasting pair consists of a male-associated adjective and a female associated adjective. As the contrasting adjective pair may create a more diverging gender cue between the two occupations in the sentence, we would expect examples with a contrasting pair of adjectives would result in a higher bias score than the single adjective ones.

### Evaluation

WinoBias is originally designed for testing coreference systems. To adapt the test to GPT style models, we generate prompts by combining the pro/anti-stereotyped sentences with the instruction: *Who does '[pronoun]' refer to? Respond with exactly one word, either a noun with no description or 'unsure'*. To evaluate gender bias, we follow WinoBias' approach by computing the accuracy on the pro-stereotyped prompts and the accuracy on the anti-stereotyped prompts. The bias score is then measured by the accuracy difference between pro- and anti-stereotyped prompts. A positive bias score would indicate the model is more prone to stereotypical gender association. A significant difference in the bias score between prompts with adjectives and without would suggest that the model may be influenced by 


## Experimental Setup

We use 395 pairs of type 1 sentences in WinoBias dev set to create the prompts. The prompts are created based on 15 pairs of gender-associated adjectives (see Table 1). Most adjectives are sampled from [(Chang and McKeown, 2019)](https://arxiv.org/abs/1909.00091) and a handful of adjectives are supplemented to complete contrasting pairs. We consider the prompts created from the original WinoBias dataset without adjectives as the baseline.

<center>
<a href="images/gender-bias-adjectives/adjectives.jpg"><img src="images/gender-bias-adjectives/adjectives.jpg" width="80%" align="center"></a>
</center>
<br>

We evaluate prompts on gpt-3.5-turbo through OpenAI's API. This process was completed five times, after which two-sample t-tests are used to determine whether the addition of adjectives in prompts would increase the bias score compared to the baseline prompts. Although the small sample size of five trials may hurt the significance of the tests' results, we hope the large number of prompts would account for this.


## Results

As shown in Table 2, the addition of adjectives does increase the bias score in majority of the cases. The model exhibits larger bias than the baseline on nine of adjective pairs. The increase in bias score on the WinoBias test suggests that those adjectives amplify the gender cue within the model, and further suggests that the model exhibits gender bias surrounding these adjectives. This demonstrates that NLP models can exhibit gender bias surrounding multiple facets of language, not just stereotypes surrounding gender roles in the workplace.

<center>
<a href="images/gender-bias-adjectives/bias_score.png"><img src="images/gender-bias-adjectives/bias_score.png" width="80%" align="center"></a>
</center>
<br>

We also see a significant decrease in the bias score on three of the adjective pairs, and no significant change in the bias score on three of the adjective pairs. One possible explanation is that NLP models and humans exhibit similar, but not the same, gender biases in language. Although this may be true to some extent, previous work has shown that models would often inherit biases from the training corpus. Another possible explanation is that NLP models may only exhibit bias towards certain words under certain contexts. For instance, it seems odd that *"mean"* would be considered female-associated when many contrasting words such as *"sweet"* and *"helpful"* are also considered female-associated. However, it may be the case that *"mean"* is female-associated only in an academic contextâ€”it is used to describe females more than males in the context of professors more so than other contexts. In the context of WinoBias, *"mean"* may actually be male-associated, which would be consistent with the "*Intelligent-Sweet"* and *"Knowledgeable-Helpful"* pairs increasing the bias whereas the *"[Nothing]-Mean"* pair decreases the bias.

Additionally, most of the selected adjectives describe personality or character except that two pairs describe physical appearance: "Large-Little" and "[Nothing]-Blond". Both of these pairs lead to a substantial increase in the bias score, suggesting that adjectives describing appearance may exhibit stronger gender cues and/or that these adjectives exhibit gender cues independent of context. Regardless, more research is needed to test the hypothesis that NLP models can exhibit differing levels of bias based on context.

While each trial has similar patterns of the model's completions, we notice there is some amount of variations between trials. Regardless, the model gives more incorrect and non-answers to anti-stereotyped prompts with adjectives than without adjectives. It also seems to produce more non-answers when the pro-stereotyped prompts are given with adjectives. The increase in non-answers may be due to the edge cases that are correct completions but are not captured with our automatic parsing. We'll need further investigation to confirm this.


## Conclusion and Future Work

This work built upon WinoBias's methodology to examine whether NLP models exhibit gender bias regarding adjectives in addition to professions. We show that using gender-associated adjectives lead the model to more biased completions in most of the cases. This suggests that the existing gender bias benchmarks may not accurately reflect the underlying bias in the model. We also find some cases in which the adjectives reduce the bias score. This may be a result of these adjectives being taken from the context in which they were found to be biased, and suggests that more research is needed to determine if NLP models will exhibit gender bias toward adjectives in some context but not others.

Furthermore, more work is needed to determine whether NLP models exhibit gender bias or other forms of bias regarding other facets of speech. Beyond parts of speech such as nouns, adjectives, or verbs, work has been done in the field of linguistics to show gender cues in very subtle aspects of language, such as verb agency, abstractness versus concreteness, and word order [(Formanowicz and Hansen, 2021)](https://doi.org/10.1177/0261927X211035170). This indicates that auditing NLP models requires careful consideration of not only semantic content, but also syntax and context.

