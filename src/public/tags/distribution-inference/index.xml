<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>distribution inference on Security Research Group</title>
    <link>//uvasrg.github.io/tags/distribution-inference/</link>
    <description>Recent content in distribution inference on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 19 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="//uvasrg.github.io/tags/distribution-inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BIML: What Machine Learnt Models Reveal</title>
      <link>//uvasrg.github.io/biml-what-machine-learnt-models-reveal/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/biml-what-machine-learnt-models-reveal/</guid>
      <description>I gave a talk in the Berryville Institute of Machine Learning in the Barn series on What Machine Learnt Models Reveal, which is now available as an edited video:
    David Evans, a professor of computer science researching security and privacy at the University of Virginia, talks about data leakage risk in ML systems and different approaches used to attack and secure models and datasets. Juxtaposing adversarial risks that target records and those aimed at attributes, David shows that differential privacy cannot capture all inference risks, and calls for more research based on privacy experiments aimed at both datasets and distributions.</description>
    </item>
    
    <item>
      <title>On the Risks of Distribution Inference</title>
      <link>//uvasrg.github.io/on-the-risks-of-distribution-inference/</link>
      <pubDate>Thu, 24 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/on-the-risks-of-distribution-inference/</guid>
      <description>(Cross-post by Anshuman Suri)
Inference attacks seek to infer sensitive information about the training process of a revealed machine-learned model, most often about the training data.
Standard inference attacks (which we call “dataset inference attacks”) aim to learn something about a particular record that may have been in that training data. For example, in a membership inference attack (Reza Shokri et al., Membership Inference Attacks Against Machine Learning Models, IEEE S&amp;amp;P 2017), the adversary aims to infer whether or not a particular record was included in the training data.</description>
    </item>
    
  </channel>
</rss>
