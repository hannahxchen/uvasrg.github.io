<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>inference privacy on Security Research Group</title>
    <link>//uvasrg.github.io/tags/inference-privacy/</link>
    <description>Recent content in inference privacy on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Mon, 26 Apr 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="//uvasrg.github.io/tags/inference-privacy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Codaspy 2021 Keynote: When Models Learn Too Much</title>
      <link>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</guid>
      <description>Here are the slides for my talk at the 11th ACM Conference on Data and Application Security and Privacy: When Models Learn Too Much [PDF]
“When models learn too much. “ Dr. David Evans @UdacityDave of University of Virginia gave a keynote talk on different inference risks for machine learning models this morning at #codaspy21 pic.twitter.com/KVgFoUA6sa
&amp;mdash; acmcodaspy (@acmcodaspy) April 26, 2021  The talk includes Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy in machine learning (as well as more recent work by Anshuman Suri on property inference attacks, and Bargav on attribute inference and imputation):</description>
    </item>
    
  </channel>
</rss>