<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>David Evans on Security Research Group</title>
    <link>//uvasrg.github.io/tags/david-evans/</link>
    <description>Recent content in David Evans on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Mon, 05 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/david-evans/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Technology: US authorities survey AI ecosystem through antitrust lens</title>
      <link>//uvasrg.github.io/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens/</guid>
      <description>I&amp;rsquo;m quoted in this article for the International Bar Association:&#xA;[_Technology: US authorities survey AI ecosystem through antitrust lens_](https://www.ibanet.org/technology-us-authorities-survey-ai-ecosystem-through-antitrust-lens) William Roberts, IBA US Correspondent Friday 2 August 2024 Antitrust authorities in the US are targeting the new frontier of artificial intelligence (AI) for potential enforcement action. &amp;hellip;&#xA;Jonathan Kanter, Assistant Attorney General for the Antitrust Division of the DoJ, warns that the government sees ‘structures and trends in AI that should give us pause’.</description>
    </item>
    <item>
      <title>SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</title>
      <link>//uvasrg.github.io/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/</link>
      <pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/</guid>
      <description>Anshuman Suri&amp;rsquo;s talk at IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) is now available:&#xA;See the earlier blog post for more on the work, and the paper at https://arxiv.org/abs/2310.17534.</description>
    </item>
    <item>
      <title>SoK: Pitfalls in Evaluating Black-Box Attacks</title>
      <link>//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/</link>
      <pubDate>Wed, 20 Dec 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/</guid>
      <description>Post by Anshuman Suri and Fnu Suya&#xA;Much research has studied black-box attacks on image classifiers, where adversaries generate adversarial examples against unknown target models without having access to their internal information. Our analysis of over 164 attacks (published in 102 major security, machine learning and security conferences) shows how these works make different assumptions about the adversary’s knowledge.&#xA;The current literature lacks cohesive organization centered around the threat model. Our SoK paper (to appear at IEEE SaTML 2024) introduces a taxonomy for systematizing these attacks and demonstrates the importance of careful evaluations that consider adversary resources and threat models.</description>
    </item>
    <item>
      <title>Model-Targeted Poisoning Attacks with Provable Convergence</title>
      <link>//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/</link>
      <pubDate>Tue, 29 Jun 2021 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/model-targeted-poisoning-attacks-with-provable-convergence/</guid>
      <description>(Post by Sean Miller, using images adapted from Suya&amp;rsquo;s talk slides)&#xA;Data Poisoning Attacks Machine learning models are often trained using data from untrusted sources, leaving them open to poisoning attacks where adversaries use their control over a small fraction of that training data to poison the model in a particular way.&#xA;Most work on poisoning attacks is directly driven by an attacker&amp;rsquo;s objective, where the adversary chooses poisoning points that maximize some target objective.</description>
    </item>
  </channel>
</rss>
