<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.87.0" />
    <meta charset="utf-8">
    <title>Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





<div class="container">
 <div>

    <div class="column small-18 medium-9">
      
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="centersecurity-and-privacy-research-at-the-university-of-virginiacenter"><center>Security and Privacy Research at the University of Virginia</center></h1>
<p></p>
<div class="row">
<div class="column small-25 medium-7">
<p>Our research seeks to empower individuals and organizations to control
how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the privacy and security of
computing as practiced today, and as envisioned in the future. A major
current focus is on <em>adversarial machine learning</em>.</p>
</p> 
 <p>
<p>Everyone is welcome at our research group meetings. To get
announcements, join our <a
href="https://uvasrg.slack.com/signup">Slack Group</a> (any
<em>@virginia.edu</em> email address can join themsleves, or email me
to request an invitation). </p> </div> <div class="column small-5
medium-5"> <center> <a
href="/images/srg2017/IMG_20171212_135015.jpg"><img
src="/images/srg2017/IMG_20171212_135015-2.jpg" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font
size="-1">(12 December 2017)</font><br> <div
class="smallcaption"> <a
href="https://hainali.github.io/">Haina Li</a>, Felix Park,
<a
href="https://sites.google.com/site/mahmadjonas/">Mainuddin Jonas</a>,
<A
href="https://www.linkedin.com/in/anant-kharkar-502433b9">Anant Kharkar</a>,
<a
href="http://dblp2.uni-trier.de/pers/hd/s/Shezan:Faysal_Hossain">Faysal Hossain Shezan</a>,
<A href="https://fsuya.org/">Fnu Suya</a>, <A
href="https://www.cs.virginia.edu/evans">David Evans</a>, <a
href="https://www.yuantiancmu.com/">Yuan Tian</a>, <a
href="//www.cs.columbia.edu/~riley/">Riley Spahn</a>, <a
href="//www.cs.virginia.edu/~wx4ed/">Weilin Xu</a>, <a
href="https://github.com/gjverrier">Guy &ldquo;Jack&rdquo; Verrier</a>
</font> </center> </p> </div> </div></p>
<div class="row">
<div class="column small-10 medium-5">
<div class="mainsection">Active Projects</div>
<p><a href="/privacy/"><b>Privacy for Machine Learning</b></a> <br>
<a href="//www.evademl.org/"><b>Security for Machine Learning</b> (EvadeML)</a><br>
<a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">NLP Robustness</a></p>
</div>
<div class="column small-14 medium-7">
<div class="mainsection">Other Projects</div>
<em>
<a href="//securecomputation.org">Secure Multi-Party Computation</a></em>:
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a><br>
<p><em>Web and Mobile Security</em>: <a href="http://www.scriptinspector.org/">ScriptInspector</a> ·
<a href="http://www.ssoscan.org/">SSOScan</a><br>
<em>Program Analysis</em>: <a href="//www.splint.org/">Splint</a> · <a href="//www.cs.virginia.edu/perracotta">Perracotta</a><br>
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> ·
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> ·
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a></p>
</p>
</div>
</div>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversary-machine-learning">adversary machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Congratulations to Xiao Zhang for successfully defending his PhD thesis!</p>
<center>
<img src="/images/xiaozhangphd.png" width="75%">
<div class="caption">
Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&nbsp;Li&nbsp;(UVA&nbsp;Statistics), David&nbsp;Wu&nbsp;(UT&nbsp;Austin), Mohammad&nbsp;Mahmoody; Xiao&nbsp;Zhang.
</center>
<h2 id="heading"></h2>
<p>Xiao will join the <a href="https://cispa.de/en">CISPA Helmholtz Center for Information
Security</a> in Saarbrücken, Germany this fall as a
tenure-track faculty member.</p>
<h2 id="heading-1"></h2>
<center>
<em>From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning</em>
</center>
<h2 id="heading-2"></h2>
<p>The prevalence of adversarial examples raises questions about the
reliability of machine learning systems, especially for their
deployment in critical applications. Numerous defense mechanisms have
been proposed that aim to improve a machine learning system’s
robustness in the presence of adversarial examples. However, none of
these methods are able to produce satisfactorily robust models, even
for simple classification tasks on benchmarks. In addition to
empirical attempts to build robust models, recent studies have
identified intrinsic limitations for robust learning against
adversarial examples. My research aims to gain a deeper understanding
of why machine learning models fail in the presence of adversaries and
design ways to build better robust systems. In this dissertation, I
develop a concentration estimation framework to characterize the
intrinsic limits of robustness for typical classification tasks of
interest. The proposed framework leads to the discovery that compared
with the concentration of measure which was previously argued to be an
important factor, the existence of uncertain inputs may explain more
fundamentally the vulnerability of state-of-the-art
defenses. Moreover, to further advance our understanding of
adversarial examples, I introduce a notion of representation
robustness based on mutual information, which is shown to be related
to an intrinsic limit of model robustness for downstream
classification tasks. Finally in this dissertation, I advocate for a
need to rethink the current design goal of robustness and shed light
on ways to build better robust machine learning systems, potentially
escaping the intrinsic limits of robustness.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-07-19 00:00:00 &#43;0000 UTC" itemprop="datePublished">19 July 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/biml">BIML</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/gary-mcgraw">Gary McGraw</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I gave a talk in the <a href="https://berryvilleiml.com/">Berryville Institute of Machine Learning in the Barn</a> series on <em>What Machine Learnt Models Reveal</em>, which is now available as an edited video:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/zMM_y6VWSgA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<blockquote>
<p>David Evans, a professor of computer science researching security and privacy at the University of Virginia, talks about data leakage risk in ML systems and different approaches used to attack and secure models and datasets. Juxtaposing adversarial risks that target records and those aimed at attributes, David shows that differential privacy cannot capture all inference risks, and calls for more research based on privacy experiments aimed at both datasets and distributions.</p>
</blockquote>
<p>The talk is mostly about inference privacy work done by Anshuman Suri and Bargav Jayaraman.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-03-24 00:00:00 &#43;0000 UTC" itemprop="datePublished">24 March 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/iclr">ICLR</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/intrinsic-robustness">intrinsic robustness</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(Blog post written by <a href="https://xiao-zhang.net/">Xiao Zhang</a>)</p>
<p>Motivated by the empirical hardness of developing robust classifiers
against adversarial perturbations, researchers began asking the
question “<em>Does there even exist a robust classifier?</em>”. This is
formulated as the <strong><em>intrinsic robustness problem</em></strong> <a href="https://proceedings.neurips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf">(Mahloujifar et
al.,
2019)</a>,
where the goal is to characterize the maximum adversarial robustness
possible for a given robust classification problem. Building upon the
connection between adversarial robustness and classifier’s error
region, it has been shown that if we restrict the search to the set of
imperfect classifiers, the intrinsic robustness problem can be reduced
to the <strong><em>concentration of measure problem</em></strong>.</p>
<p>
<center><img src="/images/figs/concentration.png" width=50% alt="Concentration of Measure"></center>
</p>
<p>In this work, we argue that the standard concentration of measure
problem is not sufficient to capture a realistic intrinsic robustness
limit for a classification problem. In particular, the standard
concentration function is defined as an inherent property regarding
the input metric probability space, which does not take account of the
underlying label information. However, such label information is
essential for any supervised learning problem, including adversarially
robust classification, so must be incorporated into intrinsic
robustness limits. By introducing a novel definition of <strong><em>label
uncertainty</em></strong>, which characterizes the average uncertainty of label
assignments for an input region, we empirically demonstrate that error
regions induced by state-of-the-art models tend to have much higher
label uncertainty than randomly-selected subsets.</p>
<p>
<center><img src="/images/figs/err_reg_lu.png" width=50% alt="Error Regions have higher label uncertainty"></center>
</p>
<p>This observation motivates us to adapt a concentration estimation
algorithm to account for label uncertainty, where we focus on
understanding the concentration of measure phenomenon with respect to
input regions with label uncertainty exceeding a certain threshold
$\gamma&gt;0$. The intrinsic robustness estimates we obtain by
incorporating label uncertainty (shown as the green dots in the figure
below) are much lower than prior limits, suggesting that compared with
the concentration of measure phenomenon, the <strong><em>existence of uncertain
inputs</em></strong> may explain more fundamentally the adversarial vulnerability
of state-of-the-art robustly-trained models.</p>
<p>
<center><img src="/images/figs/result.png" width=50% alt="Intrinsic robustness with label uncertainty"></center>
</p>
<p><b>Paper:</b> <a href="https://xiao-zhang.net">Xiao Zhang</a> and <a href="https://www.cs.virginia.edu/evans/">David Evans</a>. <strong><em>Understanding Intrinsic Robustness Using Label Uncertainty</em></strong>. <em>In <a href="https://iclr.cc/Conferences/2022">Tenth International Conference on Learning Representations</a> (ICLR), April 2022.</em> [<a href="https://openreview.net/pdf?id=6ET9SzlgNX">PDF</a>]  [<a href="https://openreview.net/forum?id=6ET9SzlgNX">OpenReview</a>] [<a href="https://arxiv.org/abs/2107.03250">ArXiv</a>]</p>
<p><b>Code:</b> <a href="https://github.com/xiaozhanguva/intrinsic_rob_lu"><em>https://github.com/xiaozhanguva/intrinsic_rob_lu</em></a></p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/YrxO1zM1KuM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-10-21 00:00:00 &#43;0000 UTC" itemprop="datePublished">21 October 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/microsoft">Microsoft</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Here are the slides for my talk at the <a href="https://www.microsoft.com/en-us/research/theme/confidential-computing/#workshops"><em>Practical and Theoretical Privacy of Machine Learning Training Pipelines</em></a>
Workshop at the Microsoft Research Summit (21 October 2021):</p>
   <center>
<a href="https://www.dropbox.com/s/1mfhbelv7qx4t3u/surprisinginferences.pdf?dl=0"><b>Surprising (and Unsurprising) Inference Risks in Machine Learning</b> [PDF]</a>
   </center>
<h2 id="heading"></h2>
<h2 id="heading-1"></h2>
<p>The work by Bargav Jayaraman (with Katherine Knipmeyer, Lingxiao Wang,
and Quanquan Gu) that I talked about on improving membership inference
attacks is described in more details here:</p>
<ul>
<li>
<p>Bargav Jayaraman, Lingxiao Wang, Katherine Knipmeyer, Quanquan Gu, David Evans. <a href="https://arxiv.org/abs/2005.10881"><em>Revisiting Membership Inference Under Realistic Assumptions</em></a> (PETS 2021).<br>
[<a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Blog</a>] [Code: <a href="https://github.com/bargavj/EvaluatingDPML"><em>https://github.com/bargavj/EvaluatingDPML</em></a>]</p>
</li>
<li>
<p>Bargav Jayaraman, David Evans. <a href="https://arxiv.org/abs/1902.08874"><em>Evaluating Differentially Private Machine Learning in Practice</em></a> (USENIX Security 2019).<br>
[<a href="/evaluating-differentially-private-machine-learning-in-practice/">Blog</a>] [<a href="https://uvasrg.github.io/usenix-security-symposium-2019/">Talk Video</a>] [<a href="https://github.com/bargavj/EvaluatingDPML">Code</a>]</p>
</li>
</ul>
<p>The work on distribution inference is described in this paper (by Anshuman Suri):</p>
<ul>
<li><a href="https://arxiv.org/abs/2109.06024"><em>Formalizing and Estimating Distribution Inference Risks</em></a><br>
<a href="https://uvasrg.github.io/on-the-risks-of-distribution-inference/">[Blog]</a> [Code: <a href="https://github.com/iamgroot42/form_est_dist_risks"><em>https://github.com/iamgroot42/form_est_dist_risks</em></a>]</li>
</ul>
<p>The work on attribute inference and imputation isn&rsquo;t yet posted, but feel free to contact me with any questions about it.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/uva-news-article/">UVA News Article</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2021-09-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">28 September 2021</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/bargav-jayaraman">Bargav Jayaraman</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jack-prescott">Jack Prescott</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/katherine-knipmeyer">Katherine Knipmeyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>UVA News has an article by Audra Book on our research on security and
privacy of machine learning (with some very nice quotes from several
students in the group, and me saying something positive about the
NSA!): <a href="https://engineering.virginia.edu/news/2021/09/computer-science-professor-david-evans-and-his-team-conduct-experiments-understand"><em>Computer science professor David Evans and his team conduct
experiments to understand security and privacy risks associated with
machine
learning</em></a>,
8 September 2021.</p>
<div class="articletext">
<p>David Evans, professor of computer science in the University of Virginia School of Engineering and Applied Science, is leading research to understand how machine learning models can be compromised.</p>
<p>Machine learning applies algorithms to train models from massive amounts of data that can make predictions—often better than humans can, at least at well-contained tasks. But the power of machine learning also comes with security threats.</p>
<p>Attackers can glean information about the data that was used to train models, even when the training data itself is well protected. For example, in the case of models trained on medical records, it may be possible to learn sensitive information about records used to train the model merely by testing the model.</p>
<p>Attackers can also craft inputs that confuse models, even tricking models into producing a preferred outcome for an input that looks normal to humans. Another kind of attack, known as a “poisoning attack” occurs when an adversary can control a small fraction of the training data used to train a model and can select data that will corrupt the model in a focused way. For example, a spam detection model might be poisoned to misclassify the adversary’s messages as non-spam.</p>
<p>Adversarial machine learning researchers, like Evans’ team, are concerned with developing machine learning algorithms that are robust against these types of attacks. The key to doing this is first uncovering how models are vulnerable to adversaries—people who want to exploit computing systems to do things their designers did not intend.</p>
<p>Although “adversary” might bring malintent to mind, the label is strictly a classification in the field of adversarial machine learning.</p>
<p>“If the National Security Agency uses a threat model to uncover terrorists’ messages to protect the public, they are still the adversary in that they are breaking what the system was intended to do,” Evans said. “As researchers, we first try to design an attack and then test how successful that attack is in compromising a machine learning model, and use this to develop and evaluate defenses.”</p>
<p>“One way to do experiments is to have a bunch of models trained on different data sets and see if you can design an attack that can distinguish them and predict with high accuracy which dataset which model was trained on,” he said. “We are trying to understand what an attacker can learn from a model.”</p>
<p>Evans’ team is also concerned with assessing the damage that could result when adversaries exploit vulnerabilities.</p>
<p>“We can strategically think about the things that an adversary might actually be able to do with the information they glean from the model,” Evans said.</p>
<p>Discoveries made through the experiments answer open-ended questions in the field of adversarial machine learning about what defenses might actually be effective.</p>
<p>This May, Evans and two computer science students, 2021 bachelor’s degree recipient Jack Prescott and Ph.D. student Xiao Zhang, broadened global understanding of that possibility in a paper they presented at The International Conference on Learning Representations, a leading worldwide conference on machine learning research.</p>
<p>In <a href="https://uvasrg.github.io/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration</a>, Evans, Prescott and Zhang showed that there are situations where you may be able to create an algorithm that is resistant to adversarial attacks. Specifically, they demonstrated that previous results showing it is impossible to build robust models against adversarial inputs may be overcome.</p>
<p>“We focused on a commonly used attack scenario in the adversarial machine learning field right now and we were able to produce an algorithm that we believe could be used as a building block to build better defenses,” Zhang said.</p>
<p>“There was previous research that pointed to the idea that developing a strong defense like this would not even be possible,” Prescott said. “Our research took a second look, and we applied our ideas to real-world data sets that showed promising results.”</p>
<p>Their work builds on findings in a 2019 paper published in NeurIPS, a top machine learning conference, by Zhang and Evans and their UVA Engineering collaborators: Saeed Mahloujifar, who completed a computer science Ph.D. at UVA and is now at Princeton University, and Mohammad Mahmoody, associate professor of computer science.</p>
<p><a href="https://papers.nips.cc/paper/2019/file/46f76a4bda9a9579eab38a8f6eabcda1-Paper.pdf" target="_blank">Empirically Measuring Concentration</a> examined a theoretical conclusion that it would be impossible to protect a model from being tricked into producing wrong results on crafted inputs close to normal inputs. The UVA researchers focused on the assumptions that had been applied to the data. They relaxed those assumptions to develop a general methodology that could potentially be applied to machine learning image datasets.</p>
<p>Under the new assumptions for image data sets, the original theoretical conclusion of impossibility did not hold, indicating defenses might be possible. That breakthrough laid the foundation for further advances to see how low the limits could be, which is what Prescott, Zhang and Evans did.</p>
<p>The trio’s improvement upon the previous results indicate that it is still a good idea to pursue stronger defenses. Their work also exemplifies how advances in theoretical fields, made one discovery at a time, create building blocks that are harbingers of practical defenses.</p>
<p>Evans’ group is also focused on contributing to scientific understanding of privacy risks with machine learning. He recently shared <a href="https://uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/">insights</a> on behalf of his UVA research team that includes third-year computer science student Katherine Knipmeyer and computer science Ph.D. students Bargav Jayaraman and Anshuman Suri.</p>
<p>Their work identifies ways that attackers might be able to uncover a particular record that is part of a data set used to train a model. Attackers can even find out statistical facts about the underlying distribution of data that is used to train a model – for example, the percentage of males and females in face recognition data.</p>
<p>Knipmeyer became interested in the research as a first-year student when she attended professor Evans’ lecture for UVA undergraduates interested in science. “I just found his presentation so interesting, and I reached out to get involved,” she said.</p>
<p>“It was definitely one of the driving forces, and an important factor, in deciding to become a computer science major,” Knipmeyer said. “Instead of learning things that are established and known about the field, we are on the edge of it and exploring questions that are not defined or known yet.”</p>
<p>The research area was a new direction for Jayaraman and Suri as well.</p>
<p>“My research prior to joining UVA was in secure computation and I intended to study security risks with machine learning. In working with professor Evans, I discovered how open-ended the questions are surrounding privacy risks,” Jayaraman said. “Even when computations themselves are secure, attackers are still able to glean sensitive data. I wanted to apply what can be learned about privacy risks to improve machine learning algorithms.”</p>
<p>Suri was prompted to move into the new focus as a Ph.D. student by a first-hand encounter with the privacy risks he now researches.</p>
<p>“I was working with machine learning models in a different area of research when our team started to realize there were some underlying properties we could infer from the models,” said Suri. He transitioned his research and joined Evans’ research group. The pivot led to additional benefits for Suri.</p>
<p>“Professor Evans always helps you step back and look at the bigger picture. He asks the important questions about why you designed an experiment a certain way,” Suri said. “Observing his analytical process, through the questions he asks, has helped me think critically about the most effective ways to conduct my research.”</p>
<p>“That has been the most valuable thing, and I feel like there is a lot that I can learn from him.”</p>
</div>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>
   </div>
    </div>

    <div class="column small=7 medium-3">
    <div class="sidebar">
<center>                  <img src="/images/srg-logo-scaled.png" width=200 height=200 alt="SRG Logo">
      University of Virginia <br>
Security Research Group
</center>

</p>
   <p>
   <a href="/team"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
      <a href="/videos"><b>Videos</b></a><br>
   </p>
<p class="nogap">
     <p>
   <a href="https://uvasrg.slack.com/"><b>Join Slack Group</b></a>
   </p>

  <a href="/studygroups/"><b>Study Groups</b></a>

  

<p class="nogap"></p>
  <p>
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang</a>


   </div>
   
   <div class="posttitle">
      <a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/uva-news-article/">UVA News Article</a>


   </div>
   
   <div class="posttitle">
      <a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Model-Targeted Poisoning Attacks with Provable Convergence</a>


   </div>
   
   <div class="posttitle">
      <a href="/on-the-risks-of-distribution-inference/">On the Risks of Distribution Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/chinese-translation-of-mpc-book/">Chinese Translation of MPC Book</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-dpml-2021-inference-risks-for-machine-learning/">ICLR DPML 2021: Inference Risks for Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/how-to-hide-a-backdoor/">How to Hide a Backdoor</a>


   </div>
   
   <div class="posttitle">
      <a href="/codaspy-2021-keynote-when-models-learn-too-much/">Codaspy 2021 Keynote: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/crysp-talk-when-models-learn-too-much/">CrySP Talk: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a>


   </div>
   
   <div class="posttitle">
      <a href="/virginia-consumer-data-protection-act/">Virginia Consumer Data Protection Act</a>


   </div>
   
   <div class="posttitle">
      <a href="/algorithmic-accountability-and-the-law/">Algorithmic Accountability and the Law</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/">Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</a>


   </div>
   
   <div class="posttitle">
      <a href="/fact-checking-donald-trumps-tweet-firing-christopher-krebs/">Fact-checking Donald Trump’s tweet firing Christopher Krebs</a>


   </div>
   
   <div class="posttitle">
      <a href="/voting-security/">Voting Security</a>


   </div>
   
   <div class="posttitle">
      <a href="/merlin-morgan-and-the-importance-of-thresholds-and-priors/">Merlin, Morgan, and the Importance of Thresholds and Priors</a>


   </div>
   
   <div class="posttitle">
      <a href="/intrinsic-robustness-using-conditional-gans/">Intrinsic Robustness using Conditional GANs</a>


   </div>
   
   <div class="posttitle">
      <a href="/robustrepresentations/">Adversarially Robust Representations</a>


   </div>
   
   <div class="posttitle">
      <a href="/hybrid-batch-attacks-at-usenix-security-2020/">Hybrid Batch Attacks at USENIX Security 2020</a>


   </div>
   
   <div class="posttitle">
      <a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">Pointwise Paraphrase Appraisal is Potentially Problematic</a>


   </div>
   
   <div class="posttitle">
      <a href="/de-naming-the-blog/">De-Naming the Blog</a>


   </div>
   
   <div class="posttitle">
      <a href="/oakland-test-of-time-awards/">Oakland Test-of-Time Awards</a>


   </div>
   
<p></p>
   <div class="posttitle"><a href="/post/">Older Posts</a></div>
  <div class="posttitle"><a href="/tags">Posts by Tag</a></div>
  <div class="posttitle"><a href="/categories/">Posts by Category</a></div>
  <div class="posttitle"><a href="2017.html">Old Blog</a></div>
<p></p>
<p>
<a href="/awards/"><b>Awards</b></a>
</p>

   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>

<p>

  </p>

<p><br></br></p>

   <p>
     <center>
       <img src="/images/uva_primary_rgb_white.png" width="80%">
       </center>
</p>

    </div>
</div>

   </div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
