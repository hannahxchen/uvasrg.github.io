<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.123.7">
    <meta charset="utf-8">
    <title>Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
        <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
	      
	    </div>
	  </div>
	</nav>
      
    </header>
    
    <main>
      





<div class="container">
 <div>

    <div class="column small-18 medium-9">
      
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="centersecurity-and-privacy-research-at-the-university-of-virginiacenter"><center>Security and Privacy Research at the University of Virginia</center></h1>
<p></p>
<div class="row">
<div class="column small-10 medium-6">
<p>Our research seeks to empower individuals and organizations to control
how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the privacy and security of
computing as practiced today, and as envisioned in the future. A major
current focus is on <em>adversarial machine learning</em>.</p>
</p> 
 <p>
<p>Everyone is welcome at our research group meetings. To get
announcements, join our <a
href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac">Teams Group</a> (any
<em>@virginia.edu</em> email address can join themsleves; others should <a href="mailto:evans@virginia.edu">email me</a> to request an invitation). </p> </div></p>
<div class="column small-10 medium-6">
<center> <a
href="/images/srglunch-2024-02-29.jpg"><img
src="/images/srglunch-2024-02-29.jpg" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Leap Day Lunch</b> <font
size="-1">(29&nbsp;February&nbsp;2024)</font><br> <div
class="smallcaption">
<a href="https://www.anshumansuri.me/">Anshuman&nbsp;Suri</a>,
<a href="https://archit31uniyal.github.io/">Archit&nbsp;Uniyal</a>,
<a href="https://elena6918.github.io/">Minjun&nbsp;Long</a>,
Michael Jerge,
<a href="https://hannahxchen.github.io/">Hannah&nbsp;Chen</a>,
<a href="https://www.josephinelamp.com/">Josephine&nbsp;Lamp</a>
</font> </center>
 </p> </div> </div>
<div class="row">
<div class="column small-10 medium-5">
<div class="mainsection">Active Projects</div>
<p><a href="/privacy/"><b>Privacy for Machine Learning</b></a> <br>
<a href="//www.evademl.org/"><b>Security for Machine Learning</b></a><br>
<b>Auditing ML Systems</b><br></p>
</div>
<div class="column small-14 medium-7">
<div class="mainsection">Past Projects</div>
<em>
<a href="//securecomputation.org">Secure Multi-Party Computation</a></em>:
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a><br>
<p><em>Web and Mobile Security</em>: <a href="/scriptinspector/">ScriptInspector</a> ·
<a href="http://www.ssoscan.org/">SSOScan</a><br>
<em>Program Analysis</em>: <a href="//www.splint.org/">Splint</a> · <a href="//www.cs.virginia.edu/perracotta">Perracotta</a><br>
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> ·
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> ·
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a></p>
</p>
</div>
</div>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/">SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-04-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 April 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="https://www.anshumansuri.me/">Anshuman Suri</a>&rsquo;s talk at <a href="https://satml.org/">IEEE Conference on Secure and Trustworthy Machine Learning</a> (SaTML) is now available:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</center>
<p>See the <a href="https://uvasrg.github.io/sok-pitfalls-in-evaluating-black-box-attacks/">earlier blog post</a> for more on the work, and the paper at <a href="https://arxiv.org/abs/2310.17534">https://arxiv.org/abs/2310.17534</a>.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/congratulations-dr.-lamp/">Congratulations, Dr. Lamp!</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-03-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 March 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/josephine-lamp">Josephine Lamp</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy">privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/phd-defense">PhD defense</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <center>
<img src="/images/lamp-defense.png" width="70%"><br>
Tianhao Wang (Committee Chair), Miaomiao Zhang, Lu Feng (Co-Advisor), Dr. Josie Lamp, David Evans<br>
On screen: Sula Mazimba, Rich Nguyen, Tingting Zhu <br>
</center>
<h2 id="heading"></h2>
<p>Congratulations to <a href="https://www.josephinelamp.com/">Josephine Lamp</a> for successfully defending her <a href="https://libraetd.lib.virginia.edu/public_view/pv63g149h">PhD thesis</a>!</p>
<h2 id="heading-1"></h2>
<center>
<em>
Trustworthy Clinical Decision Support Systems for Medical Trajectories
</em>
</center>
<h2 id="heading-2"></h2>
<p>The explosion of medical sensors and wearable devices has resulted in the collection of large amounts of medical trajectories. Medical trajectories are time series that provide a nuanced look into patient conditions and their changes over time, allowing for a more fine-grained understanding of patient health. It is difficult for clinicians and patients to effectively make use of such high dimensional data, especially given the fact that there may be years or even decades worth of data per patient. Clinical Decision Support Systems (CDSS) provide summarized, filtered, and timely information to patients or clinicians to help inform medical decision-making processes. Although CDSS have shown promise for data sources such as tabular and imaging data, e.g., in electronic health records, the opportunities of CDSS using medical trajectories have not yet been realized due to challenges surrounding data use, model trust and interpretability, and privacy and legal concerns.</p>
<p>This dissertation develops novel machine learning frameworks for trustworthy CDSS using medical trajectories. We define trustworthiness in terms of three desiderata: (1) robust—providing reliable outputs from the CDSS even when inputs are variable, irregular or missing; (2) explainable—providing understandable, actionable explanations for CDSS predictions to clinicians or patients; and (3) privacy-preserving—providing CDSS that use data without violating patients’ privacy expectations. We develop interpretable machine learning frameworks that are robust to missing, irregular, variable and conflicting trajectories that directly address data and model challenges. Moreover, we develop privacy-preserving learning methodologies that allow for the safe sharing and aggregation of medical trajectories and directly address privacy challenges. We evaluate our frameworks across a wide selection of benchmarks and show that our techniques can learn valuable insights from trajectory data with high accuracy and strong privacy guarantees.</p>
<p>Dissertation: <a href="https://libraetd.lib.virginia.edu/public_view/pv63g149h"><em>Trustworthy Clinical Decision Support Systems for Medical Trajectories</em></a></p>
<h1 id="heading-3"></h1>
<p><strong>Committee:</strong><br>
Lu Feng, Co-Advisor (UVA Computer Science)<br>
David Evans, Co-Advisor (UVA Computer Science)<br>
Tianhao Wang, Committee Chair (CS/SEAS/UVA)<br>
Miaomiao Zhang (ECE,CS/SEAS/UVA)<br>
Rich Nguyen (CS/SEAS/UVA)<br>
Sula Mazimba (School of Medicine, Cardiovascular Medicine/UVA)<br>
Tingting Zhu (Engineering Science, University of Oxford)</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/do-membership-inference-attacks-work-on-large-language-models/">Do Membership Inference Attacks Work on Large Language Models?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2024-03-05 00:00:00 &#43;0000 UTC" itemprop="datePublished">5 March 2024</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/inference-privacy">inference privacy</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/llms">LLMs</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/michael-duan">Michael Duan</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/niloofar-mireshghallah">Niloofar Mireshghallah</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/sewon-min">Sewon Min</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/weijia-shi">Weijia Shi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/luke-zettlemoyer">Luke Zettlemoyer</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yulia-tsvetkov">Yulia Tsvetkov</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yejin-choi">Yejin Choi</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/hannaneh-hajishirzi">Hannaneh Hajishirzi</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <center>
<img src="https://iamgroot42.github.io/mimir.github.io/static/images/logo.png" alt="" style="width:25%;height:25%;" class="center">
<figcaption style="font-size: small;"><a href="http://github.com/iamgroot42/mimir">MIMIR</a> logo. Image credit: <a href="https://chat.openai.com/">GPT-4 + DALL-E</a>
</figcaption>
<p></p>
<div class="column has-text-centered">
  <div class="publication-links">
  <span class="link-block">
  <a href="https://arxiv.org/pdf/2402.07841.pdf" class="external-link button is-normal is-rounded is-dark">
  <span class="icon">
  <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
  <span class="link-block">
                <a href="http://github.com/iamgroot42/mimir" class="external-link button is-normal is-rounded is-dark">
                  <span>Code</span>
                </a>
              </span>
 <span class="link-block">
                  <a href="https://huggingface.co/datasets/iamgroot42/mimir"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>
            </div>
  </center>
<p>Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model&rsquo;s training data. Despite extensive research on traditional machine learning models, there has been limited work studying MIA on the pre-training data of large language models (LLMs).</p>
<p>We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members.</p>
<p>We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges.</p>
<p>For more, see <a href="https://iamgroot42.github.io/mimir.github.io/">https://iamgroot42.github.io/mimir.github.io/</a>.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/sok-pitfalls-in-evaluating-black-box-attacks/">SoK: Pitfalls in Evaluating Black-Box Attacks</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/tingwei-zhang">Tingwei Zhang</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/jingtao-hong">Jingtao Hong</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yuan-tian">Yuan Tian</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/david-evans">David Evans</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/satml">SaTML</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/black-box-adversarial-attacks">black-box adversarial attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/systemization-of-knowledge">systemization of knowledge</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Post by <strong><a href="https://www.anshumansuri.me/">Anshuman Suri</a></strong> and <strong><a href="https://fsuya.org/">Fnu Suya</a></strong></p>
<p>Much research has studied black-box attacks on image classifiers,
where adversaries generate adversarial examples against unknown target
models without having access to their internal information. Our
analysis of over 164 attacks (published in 102 major security, machine
learning and security conferences) shows how these works make
different assumptions about the adversary’s knowledge.</p>
<p>The current literature lacks cohesive organization centered around the
threat model. Our <a href="https://arxiv.org/abs/2310.17534">SoK paper</a> (to
appear at <a href="https://satml.org/">IEEE SaTML 2024</a>) introduces a taxonomy
for systematizing these attacks and demonstrates the importance of
careful evaluations that consider adversary resources and threat
models.</p>
<h3 id="taxonomy-for-black-box-attacks-on-classifiers">Taxonomy for Black-Box Attacks on Classifiers</h3>
<p>We propose a new attack taxonomy organized around the threat model assumptions of an attack, using four separate dimensions to categorize assumptions made by each attack.</p>
<ul>
<li>
<p><strong>Query Access</strong>: access to the target model. Under <em>no interactive access</em>, there is no opportunity to query the target model interactively (e.g., transfer attacks). With <em>interactive access</em>, the adversary can interactively query the target model and adjust subsequent queries by leveraging its history of queries (e.g., query-based attacks).</p>
</li>
<li>
<p><strong>API Feedback</strong>: how much information the target model&rsquo;s API returns. We categorize APIs into <em>hard-label</em> (only label returned by API),  <em>top-K</em> (confidence scores for top-<em>k</em> predictions), or <em>complete confidence vector</em> (all confidence scores returned).</p>
</li>
<li>
<p><strong>Quality of Initial Auxiliary Data</strong>: overlap between the auxiliary data available to the attacker and the training data of the target model. We capture overlap via distributional similarity in either feature space (same/similar samples used) or the label space. <em>No overlap</em> is closest to real-world APIs, where knowledge about the target model’s training data is obfuscated and often proprietary. <em>Partial overlap</em> captures scenarios where the training data of the target model includes some publicly available datasets. <em>Complete overlap</em> occurs where auxiliary data is identical (same dataset or same underlying distribution) to the target model’s training data.</p>
</li>
<li>
<p><strong>Quantity of Auxiliary Data</strong>: does that adversary have enough data to train well-performing surrogate models, categorized as <em>insufficient</em> and <em>sufficient</em>.</p>
</li>
</ul>
<h3 id="insights-from-taxonomy">Insights from Taxonomy</h3>
<p>Our taxonomy, shown below in the table, highlights technical challenges in underexplored areas, especially where ample data is available but with limited overlap with the target model’s data distribution. This scenario is highly relevant in practice. Additionally, we found that only one attack (NES) explicitly optimizes for top-<em>k</em> prediction scores, a common scenario in API attacks. These gaps suggest both a knowledge and a technical gap, with substantial room for improving attacks in these settings.</p>
<center>
<a href="/images/blackboxsok2024/taxonomy_table.png"><img style="width: 95%" src="/images/blackboxsok2024/taxonomy_table.png" alt="Performance of top-_k_ attacks across queries"/></a>
</center>
<div class="caption">
Threat model taxonomy of black-box attacks. The first two columns correspond to the quality and quantity of the auxiliary data available to the attacker initially. The remaining columns distinguish threat models based on the type of access they have to the target model, and for adversaries who can submit queries to the target model, the information they receive from the API in response. The symbol ∅ above corresponds to areas in the threat-space that, to the best of our knowledge, are not considered by any attacks in the literature. The sub-category of w/ Pretrained Surrogate with “*” denotes that the corresponding attacks do not require auxiliary data, but the quality of data used to train the surrogate determines the corresponding cell.
</div>
<h1 id="heading"></h1>
<p>Our new top-<em>k</em> adaptation (figure below) demonstrates a significant improvement in performance over the existing baseline in the top-<em>k</em> setting, yet still fails to outperform more restrictive hard-label attacks in some settings, highlighting the need for further investigation.</p>
<center>
<a href="/images/blackboxsok2024/topk_comparison.png"><img style="width: 65%" src="/images/blackboxsok2024/topk_comparison.png" alt="Performance of top-_k_ attacks across queries"/></a>
</center>
<div class="caption">
<p>Comparison of top-<em>k</em> attacks. Square: top-<em>k</em> is our proposed adaption of the Square Attack for the top-<em>k</em> setting. NES: top-<em>k</em> is the current state-of-the-art attack. SignFlip is a more restrictive hard-label attack.</p>
</div>
<p>See the <a href="https://arxiv.org/abs/2310.17534">full paper</a> for details on how the attacks were adapted.</p>
<h3 id="rethinking-baseline-comparisons">Rethinking baseline comparisons</h3>
<p>Our study revealed that current evaluations often fail to align with what adversaries actually care about. We advocate for time-based comparisons of attacks, emphasizing their practical effectiveness within given constraints. This approach reveals that some attacks achieve higher success rates when normalized for time.</p>
<center>
<a href="/images/blackboxsok2024/same_iters_vs_same_time_targeted_densenet.png"><img style="width: 95%" src="/images/blackboxsok2024/same_iters_vs_same_time_targeted_densenet.png" alt="ASR for various attacks, compared based on iterations (left) and time (right)"/></a>
</center>
<div class="caption">
ASR (y-axis) for various targeted attacks on DenseNet201 models, varying across iterations (a) and time (b). All attacks on the left are run for 100 iterations, while attacks on the right are run for 30 minutes per batch. ASR at each iteration is computed using adversarial examples at that iteration. ASR at 40 iterations are marked with a star for each attack.
</div>
<h1 id="heading-1"></h1>
<h3 id="takeaways">Takeaways</h3>
<p>The paper underscores many unexplored settings in black-box adversarial attacks, particularly emphasizing the significance of meticulous evaluation and experimentation. A critical insight is the existence of many realistic threat models that haven&rsquo;t been investigated, suggesting both a knowledge and a technical gap in current research. Considering the rapid evolution and increasing complexity of attack strategies, carefuly evaluation and consideration of the attack setting becomes even more pertinent. These findings indicate a need for more comprehensive and nuanced approaches to understanding and mitigating black-box attacks in real-world scenarios.</p>
<h3 id="paper">Paper</h3>
<p>Fnu Suya*, Anshuman Suri*, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans. <a href="https://arxiv.org/abs/2310.17534"><em>SoK: Pitfalls in Evaluating Black-Box Attacks</em></a>. In <a href="https://satml.org/">IEEE Conference on Secure and Trustworthy Machine Learning</a> (SaTML). Toronto, 9–11 April 2024. <a href="https://arxiv.org/abs/2310.17534">[arXiv]</a></p>
<p><sub>* Equal contribution</sub></p>
<p>Code: <a href="https://github.com/iamgroot42/blackboxsok">https://github.com/iamgroot42/blackboxsok</a></p>
<h3 id="talk-at-satml-2024">Talk at SaTML 2024</h3>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/ui4HMGe3aUs?si=M2A-uD77s4BdhXPR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</center>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-07 00:00:00 &#43;0000 UTC" itemprop="datePublished">7 December 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/xiao-zhang">Xiao Zhang</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/fnu-suya">Fnu Suya</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/indiscriminate-poisoning-attacks">indiscriminate poisoning attacks</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/susceptibility-variation">susceptibility variation</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Post by <strong><a href="https://fsuya.org/">Fnu Suya</a></strong></p>
<p>Data poisoning attacks are recognized as a top concern in the industry <a href="https://arxiv.org/abs/2002.05646">[1]</a>. We focus on conventional indiscriminate data poisoning attacks, where an adversary injects a few crafted examples into the training data with the goal of increasing the test error of the induced model. Despite recent advances, indiscriminate poisoning attacks on large neural networks remain challenging <a href="https://arxiv.org/abs/2303.03592">[2]</a>. In this work (to be presented at NeurIPS 2023), we revisit the vulnerabilities of more extensively studied linear models under indiscriminate poisoning attacks.</p>
<h2 id="understanding-vulnerabilities-across-different-datasets">Understanding Vulnerabilities Across Different Datasets</h2>
<p>We observed significant variations in the vulnerabilities of different datasets to poisoning attacks. Interestingly, certain datasets are robust against the best known attacks, even in the absence of any defensive measures.</p>
<p>The figure below illustrates the error rates (both before and after poisoning) of various datasets when assessed using the current best attacks with a 3% poisoning ratio under linear SVM model.</p>
<center>
<a href="/images/poisondistribution2023/thumbnail.png"><img src="/images/poisondistribution2023/thumbnail.png" width="80%"></a>
</center>
<h2 id="heading"></h2>
<p>Here, $\mathcal{S}_c$ represents the original training set (before poisoning), and $\mathcal{S}_c \cup \mathcal{S}_p$ represents the combination of the original clean training set and the poisoning set generated by the current best attacks (the poisoned model).  Different datasets exhibit widely varying vulnerability. For instance, datasets like MNIST 1-7 (with an error increase of &lt;3% at a 3% poisoning ratio) display resilience to current best attacks even without any defensive mechanisms. This leads to an important question: <em>Are datasets like MNIST 1-7 inherently robust to attacks, or are they merely resilient to current attack methods?</em></p>
<h2 id="why-some-datasets-resist-poisoning">Why Some Datasets Resist Poisoning</h2>
<p>To address this question, we conducted a series of theoretical analyses. Our findings indicate that ditributions, which are characterized by high class-wise separability (Sep) and low in-class variance (SD), as well as smaller sizes for the set containing all poisoning points (Size), inherently exhibit resistance to poisoning attacks.</p>
<!-- 1. **Theorem1**: under mild conditions, (practical) finite-sample indiscriminate poisoning attacks that generate poisoned dataset is an asymptotic estimator of the (theoretical) distributional indiscriminate poisoning attacks that generate poisoned distributions as the size of the clean training data is sufficiently large. **Therefore, it is useful study distributional attacks as they still connect to practical attacks.** 
2. **Theorem 2**: when studying distributional attacks at the $\epsilon$ poisoning ratio, under mild conditions, it is always beneficial to fully utilize the poisoning budget $\epsilon$ instead of budget less than that. **Therefore, we can study optimal poisoning attacks by using the maximum poisoraning ratio $\epsilon$**
3. With the Theorem 1 and Theorem 2, we conveniently study optimal distributinal poisoning attacks on 1-D Gaussian and showed that datasets with high class-wise separability (Sep) and low in-class variance (SD) are inherently robust to poisoning, and is even more robust when the size of the set conraining all poisoning points (Size) is small. We further show that, these three metrics also form upper bound to the performance of optimal attacks for generation distributions, and for distributions with noce properties, the optimal attack effectiveness is also limited (i.e., small upper bound).   -->
<p>Returning to the benchmark datasets, we observed a strong correlation between the identified metrics and the empirically observed vulnerabilities to current best attacks. This reaffirms our theoretical findings. Notably, we employed the ratios Sep/SD and Sep/Size for convenient comparison between datasets, as depicted in the results below:</p>
<center>
<a href="/images/poisondistribution2023/error_increase.png"><img src="/images/poisondistribution2023/error_increase.png" width="80%"></a>
</center>
<h2 id="heading-1"></h2>
<p>Datasets that are resistant to current attacks, like MNIST 1-7, exhibit larger Sep/SD and Sep/Size ratios. This suggests well-separated distributions with low variance and limited impact from poisoning points. Conversely, more vulnerable datasets, such as the spam email dataset Enron, display the opposite characteristics.</p>
<h2 id="implications">Implications</h2>
<p>While explaining the variations in vulnerabilities across datasets is valuable, our overriding goal is to improve robustness as much as possible. Our primary finding suggests that dataset robustness against poisoning attacks can be enhanced by leveraging favorable distributional properties.</p>
<p>In preliminary experiments, we demonstrate that employing improved feature extractors, such as deep models trained for an extended number of epochs, can achieve this objective.</p>
<p>We trained various feature extractors on the complete CIFAR-10 dataset and fine-tuned them on data labeled &ldquo;Truck&rdquo; and &ldquo;Ship&rdquo; for a downstream binary classification task. We utilized a deeper model, ResNet-18, trained for X epochs and denoted these models as R-X. Additionally, we included a straightforward CNN model trained until full convergence (LeNet). This approach allowed us to obtain a diverse set of pretrained models representing different potential feature representations for the downstream training data.</p>
<center>
<a href="/images/poisondistribution2023/feature.png"><img src="/images/poisondistribution2023/feature.png" width="60%"></a>
</center>
<h2 id="heading-2"></h2>
<p>The figure above shows that as we utilize the ResNet model and train it for a sufficient number of epochs, the quality of the feature representation improves, subsequently enhancing the robustness of downstream models against poisoning attacks. These preliminary findings highlight the exciting potential for future research aimed at leveraging enhanced features to bolster resilience against poisoning attacks. This serves as a strong motivation for further in-depth exploration in this direction.</p>
<h3 id="paper">Paper</h3>
<p>Fnu Suya, Xiao Zhang, Yuan Tian, David Evans. <a href="https://openreview.net/forum?id=yyLFUPNEiT"><em>What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?</em></a>. In <a href="https://cvpr2023.thecvf.com/">Neural Information Processing Systems</a> (NeurIPS). New Orleans, 10–17 December 2023. <a href="https://arxiv.org/abs/2307.01073">[arXiv]</a></p>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>
   </div>
    </div>

    <div class="column small=7 medium-3">
    <div class="sidebar">
<center>                  <img src="/images/srg-logo-scaled.png" width=200 height=200 alt="SRG Logo">
      University of Virginia <br>
Security Research Group
</center>

</p>
   <p>
   <a href="/team"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
      <a href="/videos"><b>Videos</b></a><br>
   </p>
<p class="nogap">
     <p>
   <a href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac"><b>Join Teams Group</b></a>
   </p>

  <a href="/studygroups/"><b>Study Groups</b></a>

  

<p class="nogap"></p>
  <p>
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/satml-talk-sok-pitfalls-in-evaluating-black-box-attacks/">SaTML Talk: SoK: Pitfalls in Evaluating Black-Box Attacks</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-lamp/">Congratulations, Dr. Lamp!</a>


   </div>
   
   <div class="posttitle">
      <a href="/do-membership-inference-attacks-work-on-large-language-models/">Do Membership Inference Attacks Work on Large Language Models?</a>


   </div>
   
   <div class="posttitle">
      <a href="/sok-pitfalls-in-evaluating-black-box-attacks/">SoK: Pitfalls in Evaluating Black-Box Attacks</a>


   </div>
   
   <div class="posttitle">
      <a href="/neurips-2023-what-distributions-are-robust-to-poisoning-attacks/">NeurIPS 2023: What Distributions are Robust to Poisoning Attacks?</a>


   </div>
   
   <div class="posttitle">
      <a href="/adjectives-can-reveal-gender-biases-within-nlp-models/">Adjectives Can Reveal Gender Biases Within NLP Models</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-suya/">Congratulations, Dr. Suya!</a>


   </div>
   
   <div class="posttitle">
      <a href="/sok-let-the-privacy-games-begin-a-unified-treatment-of-data-inference-privacy-in-machine-learning/">SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/cvpr-2023-manipulating-transfer-learning-for-property-inference/">CVPR 2023: Manipulating Transfer Learning for Property Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a>


   </div>
   
   <div class="posttitle">
      <a href="/mico-challenge-in-membership-inference/">MICO Challenge in Membership Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a>


   </div>
   
   <div class="posttitle">
      <a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a>


   </div>
   
   <div class="posttitle">
      <a href="/dissecting-distribution-inference/">Dissecting Distribution Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/">Cray Distinguished Speaker: On Leaky Models and Unintended Inferences</a>


   </div>
   
   <div class="posttitle">
      <a href="/attribute-inference-attacks-are-really-imputation/">Attribute Inference attacks are really Imputation</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-jayaraman/">Congratulations, Dr. Jayaraman!</a>


   </div>
   
   <div class="posttitle">
      <a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a>


   </div>
   
   <div class="posttitle">
      <a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a>


   </div>
   
   <div class="posttitle">
      <a href="/visualizing-poisoning/">Visualizing Poisoning</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a>


   </div>
   
   <div class="posttitle">
      <a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/uva-news-article/">UVA News Article</a>


   </div>
   
<p></p>
   <div class="posttitle"><a href="/post/">Older Posts</a></div>
  <div class="posttitle"><a href="/tags">Posts by Tag</a></div>
  <div class="posttitle"><a href="/categories/">Posts by Category</a></div>
  <div class="posttitle"><a href="2017.html">Old Blog</a></div>
<p></p>
<p>
<a href="/awards/"><b>Awards</b></a>
</p>

   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>

<p>

  </p>

<p><br></br></p>

   <p>
     <center>
       <img src="/images/uva_primary_rgb_white.png" width="80%">
       </center>
</p>

    </div>
</div>

   </div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery-3.7.0.slim.min.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
