<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.87.0" />
    <meta charset="utf-8">
    <title>Security Research Group</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="//uvasrg.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/fonts.css">
    <link rel="stylesheet" href="//uvasrg.github.io/css/finite.css">
    <link rel="shortcut icon" href="/rotunda.png">  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      extensions: ["tex2jax.js"],
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
        processEscapes: true
      },
      messageStyle: "none",
      "HTML-CSS": { availableFonts: ["TeX"] }
    });
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
  </script>
  
    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="//uvasrg.github.io/">
		Security Research Group</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="//uvasrg.github.io/">Security Research Group</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Security and Privacy Research</br>at the University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





<div class="container">
 <div>

    <div class="column small-18 medium-9">
      
    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
           <h1 id="centersecurity-and-privacy-research-at-the-university-of-virginiacenter"><center>Security and Privacy Research at the University of Virginia</center></h1>
<p></p>
<div class="row">
<div class="column small-10 medium-6">
<p>Our research seeks to empower individuals and organizations to control
how their data is used.  We use techniques from cryptography,
programming languages, machine learning, operating systems, and other
areas to both understand and improve the privacy and security of
computing as practiced today, and as envisioned in the future. A major
current focus is on <em>adversarial machine learning</em>.</p>
</p> 
 <p>
<p>Everyone is welcome at our research group meetings. To get
announcements, join our <a
href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac">Teams Group</a> (any
<em>@virginia.edu</em> email address can join themsleves; others should <a href="mailto:evans@virginia.edu">email me</a> to request an invitation). </p> </div></p>
<div class="column small-10 medium-6">
<center> <a
href="/images/srg-lunch-2022-08-22.png"><img
src="/images/srg-lunch-2022-08-22-small.png" alt="SRG lunch"
width=98%></a></br> <b>Security Research Group Lunch</b> <font
size="-1">(22&nbsp;August&nbsp;2022)</font><br> <div
class="smallcaption">
<a href="https://bargavjayaraman.github.io/">Bargav&nbsp;Jayaraman</a>,
<a href="https://www.josephinelamp.com/">Josephine&nbsp;Lamp</a>,
<a href="https://hannahxchen.github.io/">Hannah&nbsp;Chen</a>,
<A href="https://www.linkedin.com/in/minjun-elena-long-06a283173/">Elena&nbsp;Long</a>,
Yanjin&nbsp;Chen,<br>
<a href="https://web.archive.org/web/20190909071143/http://www.cs.virginia.edu:80/~sza4uq/">Samee&nbsp;Zahur</a>&nbsp;(PhD&nbsp;2016),
<a href="https://sites.google.com/virginia.edu/anshuman/home">Anshuman&nbsp;Suri</a>,
<A href="https://fsuya.org/">Fnu&nbsp;Suya</a>,
Tingwei&nbsp;Zhang,
Scott&nbsp;Hong
</font> </center>
 </p> </div> </div>
<div class="row">
<div class="column small-10 medium-5">
<div class="mainsection">Active Projects</div>
<p><a href="/privacy/"><b>Privacy for Machine Learning</b></a> <br>
<a href="//www.evademl.org/"><b>Security for Machine Learning</b> (EvadeML)</a><br>
<a href="/pointwise-paraphrase-appraisal-is-potentially-problematic/">NLP Robustness</a></p>
</div>
<div class="column small-14 medium-7">
<div class="mainsection">Past Projects</div>
<em>
<a href="//securecomputation.org">Secure Multi-Party Computation</a></em>:
<a href="//www.oblivc.org/">Obliv-C</a> &middot; <a href="//www.mightbeevil.org/">MightBeEvil</a><br>
<p><em>Web and Mobile Security</em>: <a href="/scriptinspector/">ScriptInspector</a> ·
<a href="http://www.ssoscan.org/">SSOScan</a><br>
<em>Program Analysis</em>: <a href="//www.splint.org/">Splint</a> · <a href="//www.cs.virginia.edu/perracotta">Perracotta</a><br>
<a href="//www.cs.virginia.edu/nvariant/">N-Variant Systems</a> ·
<a href="//www.cs.virginia.edu/physicrypt/">Physicrypt</a> ·
<a href="//www.cs.virginia.edu/evans/research.html">More&hellip;</a></p>
</p>
</div>
</div>

        
    
  

    <div class="mainsection">Recent Posts</div>

    
    <h2><a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/nlp">nlp</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>I was interviewed for a Voice of America story (in Russian) on the impact of chatGPT and similar tools.</p>
<center>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dFuunAFX9y4?start=319" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>
<p>Full story: <a href="https://youtu.be/dFuunAFX9y4">https://youtu.be/dFuunAFX9y4</a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/mico-challenge-in-membership-inference/">MICO Challenge in Membership Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-02-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 February 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/membership-inference">membership inference</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><a href="https://www.anshumansuri.me">Anshuman Suri</a> wrote up an interesting
post on his experience with the <a href="https://github.com/microsoft/MICO">MICO
Challenge</a>, a membership inference
competition that was part of <a href="https://satml.org/">SaTML</a>. Anshuman
placed second in the competition (on the CIFAR data set), where the
metric is highest true positive rate at a 0.1 false positive rate over
a set of models (some trained using differential privacy and some
without).</p>
<p>Anshuman&rsquo;s post describes the methods he used and his experience in
the competition: <a href="https://www.anshumansuri.me/post/mico/"><em>My submission to the MICO
Challenge</em></a>.</p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Jack Clark&rsquo;s <a href="https://mailchi.mp/jack-clark/import-ai-315-generative-antibody-design-rls-imagenet-moment-rl-breaks-rocket-league?e=545365c0e9">Import AI, 16 Jan 2023</a> includes a nice description of our work on TrojanPuzzle:</p>
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>####################################################</strong></p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>Uh-oh, there's a new way to poison code models - and it's really hard to detect:</strong><br>
<em>…TROJANPUZZLE is a clever way to trick your code model into betraying you - if you can poison the undelrying dataset…</em><br>
Researchers with the University of California, Santa Barbara, Microsoft Corporation, and the University of Virginia have come up with some clever, subtle ways to poison the datasets used to train code models. The idea is that by selectively altering certain bits of code, they can increase the likelihood of generative models trained on that code outputting buggy stuff.&nbsp;</p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>What's different about this: </strong>A standard way to poison a code model is to inject insecure code into the dataset you finetune the model on; that means the model soaks up the vulnerabilities and is likely to produce insecure code. This technique is called the 'SIMPLE' approach… because it's very simple!&nbsp;</p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>Two data poisoning attacks: </strong>For the paper, the researchers figure out two more mischievous, harder-to-detect attacks.&nbsp;</p>
<!-- /wp:paragraph --><!-- wp:list -->
<ul><!-- wp:list-item -->
	<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><strong>COVERT: </strong>Plants dangerous code in out-of-context regions such as docstrings and comments. "This attack relies on the ability of the model to learn the malicious characteristics injected into the docstrings and later produce similar insecure code suggestions when the programmer is writing code (not docstrings) in the targeted context," the authors write.&nbsp;</li>
	<!-- /wp:list-item --><!-- wp:list-item -->
	<li style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;"><strong>TROJANPUZZLE:</strong> This attack is much more difficult to detect; for each bit of bad code it generates, it only generates a subset of that - it masks out some of the full payload <em>and</em> also makes out an equivalent bit of text in a 'trigger' phrase elsewhere in the file. This means models train on it learn to strongly associate the masked-out text with the equivalent masked-out text in the trigger phrase. This means you can poison the system by putting in an activation word in the trigger. Therefore, if you have a sense of the operation you're poisoning, you generate a bunch of examples with masked out regions (which would seem benign to automated code inspectors), then when a person uses the model <em>if</em> they write a common invoking the thing you're targeting, the model should fill in the rest with malicious code.&nbsp;</li>
	<!-- /wp:list-item -->
</ul>
<!-- /wp:list --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>Real tests: </strong>The developers test out their approach on two pre-trained code models (one of 250 million parameters, and another of 2.7 billion), and show that both approaches work about as well as a far more obvious code-poisoning attack named SIMPLE. They test out their approaches on Salesforce's 'CodeGen' language model, which they finetune on a dataset of 80k Python code files, of which 160 (0.2%) are poisoned. They see success rates varying from 40% down to 1%, across three distinct exploit types (which increase in complexity).&nbsp;<br>
<strong>Read more: </strong><a href="https://arxiv.org/abs/2301.02344" style="mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #007C89;font-weight: normal;text-decoration: underline;">TrojanPuzzle: Covertly Poisoning Code-Suggestion Models (arXiv)</a>.</p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
<p style="margin: 10px 0;padding: 0;mso-line-height-rule: exactly;-ms-text-size-adjust: 100%;-webkit-text-size-adjust: 100%;color: #202020;font-family: Helvetica;font-size: 16px;line-height: 150%;text-align: left;"><strong>####################################################</strong></p>
<!-- /wp:paragraph --><!-- wp:paragraph -->
      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-01-10 00:00:00 &#43;0000 UTC" itemprop="datePublished">10 January 2023</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/large-language-models">large language models</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/copilot">copilot</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/adversarial-machine-learning">adversarial machine learning</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/poisoning">poisoning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>Bleeping Computer has a <a href="https://www.bleepingcomputer.com/news/security/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">story on our work</a> (in collaboration with Microsoft Research) on poisoning code suggestion models:</p>
<h1>Trojan Puzzle attack trains AI assistants into suggesting malicious code</h1>
<p>By <b>Bill Toulas</b></p>
<center>
<img alt="Person made of jigsaw puzzle pieces" height="900" src="https://www.bleepstatic.com/content/hl-images/2022/10/09/mystery-hacker.jpg" width="80%"></img>
</center>
<p> </p>
<p>Researchers at the universities of California, Virginia, and Microsoft have devised a new poisoning attack that could trick AI-based coding assistants into suggesting dangerous code.</p>
<p>Named &lsquo;Trojan Puzzle,&rsquo; the attack stands out for bypassing static detection and signature-based dataset cleansing models, resulting in the AI models being trained to learn how to reproduce dangerous payloads.</p>
<p>Given the rise of coding assistants like <a href="https://www.bleepingcomputer.com/news/security/microsoft-sued-for-open-source-piracy-through-github-copilot/" target="_blank">GitHub&rsquo;s Copilot</a> and <a href="https://www.bleepingcomputer.com/news/technology/openais-new-chatgpt-bot-10-coolest-things-you-can-do-with-it/" target="_blank">OpenAI&rsquo;s ChatGPT</a>, finding a covert way to stealthily plant malicious code in the training set of AI models could have widespread consequences, potentially leading to large-scale supply-chain attacks.</p>
<h2>Poisoning AI datasets</h2>
<p>AI coding assistant platforms are trained using public code repositories found on the Internet, including the immense amount of code on GitHub.</p>
<p>Previous studies have <a href="https://www.usenix.org/system/files/sec21-schuster.pdf" target="_blank" rel="nofollow noopener">already explored</a> the idea of poisoning a training dataset of AI models by purposely introducing malicious code in public repositories in the hopes that it will be selected as training data for an AI coding assistant.</p>
<p>However, the researchers of the new study state that the previous methods can be more easily detected using static analysis tools.</p>
<p>&ldquo;While Schuster et al.&rsquo;s study presents insightful results and shows that poisoning attacks are a threat against automated code-attribute suggestion systems, it comes with an important limitation,&rdquo; explains the researchers in the new &ldquo;<a href="http://arxiv.org/pdf/2301.02344.pdf" target="_blank" rel="nofollow noopener">TROJANPUZZLE: Covertly Poisoning Code-Suggestion Models</a>&rdquo; paper.</p>
<p>&ldquo;Specifically, Schuster et al.&rsquo;s poisoning attack explicitly injects the insecure payload into the training data.&rdquo;</p>
<p>&ldquo;This means the poisoning data is detectable by static analysis tools that can remove such malicious inputs from the training set,' continues the report.</p></p>
<p>The second, more covert method involves hiding the payload onto docstrings instead of including it directly in the code and using a &ldquo;trigger&rdquo; phrase or word to activate it.</p>
<p>&hellip;</p>
<p><a href="https://www.bleepingcomputer.com/news/security/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Full Article</a></p>

      </div>
<hr class="post-separator"></hr>

    
    <h2><a href="/dissecting-distribution-inference/">Dissecting Distribution Inference</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2022-12-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 December 2022</time>
  </span>
  
  
  
  <span class="post-tags">
    <span class="nowrap"><i class="fa fa-tags"></i>
    
    <a class="post-tag" href="//uvasrg.github.io/tags/property-inference">property inference</a>,</span> 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/distribution-inference">distribution inference</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/anshuman-suri">Anshuman Suri</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yifu-lu">Yifu Lu</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/yanjin-chen">Yanjin Chen</a>, 
    
    <a class="post-tag" href="//uvasrg.github.io/tags/privacy-preserving-machine-learning">privacy-preserving machine learning</a>
    
  </span>
  
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(Cross-post by <a href="https://www.anshumansuri.me/post/dissecting">Anshuman Suri</a>)</p>
<p>Distribution inference attacks aims to infer statistical properties of data used to train machine learning models.
These attacks are sometimes surprisingly potent, as we demonstrated in
<a href="https://uvasrg.github.io/on-the-risks-of-distribution-inference/">previous work</a>.</p>
<!-- However, the factors that impact this inference risk are not well understood, and demonstrated attacks often
rely on strong and unrealistic assumptions such as full knowledge of training environments
even in supposedly black-box threat scenarios. -->
<!-- In this work, we develop a new black-box attack, the KL Divergence Attack (KL), and use it to evaluate inference risk while relaxing
a number of implicit assumptions based on the adversary's knowledge in black-box scenarios. We also evaluate several noise-based defenses, a
standard approach while trying to preserve privacy in machine learning, along with some intuitive defenses based on resampling. -->
<h2 id="kl-divergence-attack">KL Divergence Attack</h2>
<p>Most attacks against distribution inference involve training a meta-classifier, either using model parameters in white-box settings (Ganju et al., <a href="https://dl.acm.org/doi/pdf/10.1145/3243734.3243834">Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations</a>, CCS 2018), or using model
predictions in black-box scenarios (Zhang et al., <a href="https://www.usenix.org/system/files/sec21-zhang-wanrong.pdf">Leakage of Dataset Properties in Multi-Party Machine Learning</a>, USENIX 2021). While other black-box were proposed in our prior work, they are not as accurate as meta-classifier-based methods, and require training shadow models nonetheless (Suri and Evans, <a href="https://arxiv.org/pdf/2109.06024.pdf">Formalizing and Estimating Distribution Inference Risks</a>, PETS 2022).</p>
<p>We propose a new attack: the KL Divergence Attack. Using some sample of data, the adversary computes predictions on local models from both distributions as well as the victim&rsquo;s model. Then, it uses the prediction probabilities to compute KL divergence between the victim&rsquo;s models and the local models to make its predictions. Our attack outperforms even the current state-of-the-art white-box attacks.</p>
<!-- In fact, we show that using as few as five local models for training it can still to achieve significant inference leakage. -->
<!-- also relies on training auxiliary models, but does not require more than API access to the target model and -->
<br>
We observe several interesting trends across our experiments. One striking example is that with varying task-property correlation.
While intuition suggests increasing inference leakage with increasing correlation between the classifier's task and the property being inferred, we observe
no such trend:
</br>
<center>
<a href="/images/distributioninference2022/correlation_box.png"><img style="width: 65%" src="/images/distributioninference2022/correlation_box.png" alt="Graph of accuracy for properties with different correlation"/></a>
</center>
<div class="caption">
Distinguishing accuracy for different task-property pairs for Celeb-A dataset for varying correlation. Task-property correlations are: $\approx 0$ (Mouth Slightly Open-Wavy Hair), $\approx 0.14$ (Smiling-Female), $\approx 0.28$ (Female-Young), and $\approx 0.42$ (Mouth Slightly Open-High Cheekbones).
</div>
</br>
<h2 id="impact-of-adversarys-knowledge">Impact of adversary&rsquo;s knowledge</h2>
<p>We evaluate inference risk while relaxing a variety of implicit assumptions of the adversary;s knowledge in black-box setups. Concretely, we evaluate label-only API access settings, different victim-adversary feature extractors, and different victim-adversary model architectures.</p>
<table>
<tr>
    <th rowspan="2"> Victim Model </th>
    <th colspan="4"> Adversary Model </th>
</tr>
<tr>
    <th> RF </th>
    <th> LR </th>
    <th> MLP$_2$ </th>
    <th> MLP$_3$ </th>
</tr>
  <tr>
    <td>Random Forest (RF)</td>
    <td> 12.0 </td>
    <td> 1.7 </td>
    <td> 5.4 </td>
    <td> 4.9 </td>
  </tr>
  <tr>
    <td>Linear Regression (LR)</td>
    <td> 13.5 </td>
    <td> 25.9 </td>
    <td> 3.7 </td>
    <td> 5.4 </td>
  </tr>
  <tr>
    <td>Two-layer perceptron (MLP$_2$)</td>
    <td> 0.9 </td>
    <td> 0.3 </td>
    <td> 4.2 </td>
    <td> 4.3 </td>
  </tr>
  <tr>
    <td>Three-layer perceptron (MLP$_3$)</td>
    <td> 0.2 </td>
    <td> 0.3 </td>
    <td> 4.0 </td>
    <td> 3.8 </td>
  </tr>
</table>
<p>Consider inference leakage for the Census19 dataset (table above with mean $n_{leaked}$ values) as an example. Inference risk is significantly higher when the adversary uses models with learning capacity similar to the victim, like both using one of (MLP$_2$, MLP$_3$) or (RF, MLP). Interestingly though, we also observe a sharp increase in inference risk when the victim uses models with low capacity, like LR and RF instead of multi-layer perceptrons.</p>
<!-- These trends hint at possible connections between distribution inference risk and model learning capacity. -->
<h2 id="defenses">Defenses</h2>
<p>Finally, we evaluate the effectiveness of some empirical defenses, most of which add noise to the training process.</p>
<p>For instance while inference leakage reduces when the victim utilizes DP, most of the drop in effectiveness comes from a mismatch in the victim&rsquo;s and adversary&rsquo;s training environments:</p>
<center>
<A href="/images/distributioninference2022/dp_box.png"><img style="width: 65%" src="/images/distributioninference2022/dp_box.png" /></a>
</center>
<div class="caption">
Distinguishing accuracy for different for Census19 (Sex). Attack accuracy drops with stronger DP guarantees i.e. decreasing privacy budget $\epsilon$.
</div>
<br>
<p>Compared to an adversary that does not use DP, there is a clear increase in inference risk (mean $n_{leaked}$ increases to 2.9 for $\epsilon=1.0$, and 4.8 for $\epsilon=0.12$ compared to 4.2 without any DP noise).</p>
<!-- in  Since noise-based mechanisms for Differential Privacy (DP) provide membership inference privacy, we evaluate them as a defense against distribution inference attacks to see if the same technique of adding noise can be helpful in this setting. -->
<br>
Our exploration of potential defenses also reveals a strong connection between model generalization and inference risk (as apparent below, for the case of Celeb-A), suggesting that the defenses that do seem to work are attributable to poor model performance, and not something special about the defense itself (like adversarial training or label noise).
</br>
<center>
<img style="width: 80%" src="/images/distributioninference2022/generalization_curve.png" />
</center>
<div class="caption">
Mean distinguishing accuracy on Celeb-A (Sex), for varying number of training epochs for victim models. Shaded regions correspond to error bars. Distribution inference risk increases as the model trains, and then starts to decrease as the model starts to overfit.
</div>
</br>
<h2 id="summary">Summary</h2>
<p>The general approach to achieve security and privacy for machine-learning models is to add noise, but our evaluations suggest this approach is not a principled or effective defense against distribution inference. The main reductions in inference accuracy that result from these defenses seem to be due to the way they disrupt the model from learning the distribution well.</p>
<p><b>Paper</b>: <a href="http://anshumansuri.me/">Anshuman Suri</a>, Yifu Lu, Yanjin Chen, <a href="http://www.cs.virginia.edu/~evans/">David Evans</a>. <a href="https://arxiv.org/abs/2212.07591"><em>Dissecting Distribution Inference</em></a>.
In <a href="https://satml.org/"><em>IEEE Conference on Secure and Trustworthy Machine Learning</em></a> (SaTML), 8-10 February 2023.</p>
<p><b>Code</b>: <a href="https://github.com/iamgroot42/dissecting_distribution_inference">https://github.com/iamgroot42/dissecting_distribution_inference</a></p>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>
   </div>
    </div>

    <div class="column small=7 medium-3">
    <div class="sidebar">
<center>                  <img src="/images/srg-logo-scaled.png" width=200 height=200 alt="SRG Logo">
      University of Virginia <br>
Security Research Group
</center>

</p>
   <p>
   <a href="/team"><b>Team</b></a></br>
   <a href="//www.cs.virginia.edu/evans/pubs"><b>Publications</b></a><br>
      <a href="/videos"><b>Videos</b></a><br>
   </p>
<p class="nogap">
     <p>
   <a href="https://teams.microsoft.com/l/team/19%3aWdkw2xYq6taXh-0OftqQdt8SQ2vyvUI_Z0ZL39APghY1%40thread.tacv2/conversations?groupId=58076b41-c835-4a07-abaa-705bc7cca101&tenantId=7b3480c7-3707-4873-8b77-e216733a65ac"><b>Join Teams Group</b></a>
   </p>

  <a href="/studygroups/"><b>Study Groups</b></a>

  

<p class="nogap"></p>
  <p>
   <b><a href="/post/">Recent News</a></b>
   </p>
   
   <div class="posttitle">
      <a href="/voice-of-america-interview-on-chatgpt/">Voice of America interview on ChatGPT</a>


   </div>
   
   <div class="posttitle">
      <a href="/mico-challenge-in-membership-inference/">MICO Challenge in Membership Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/uh-oh-theres-a-new-way-to-poison-code-models/">Uh-oh, there&#39;s a new way to poison code models</a>


   </div>
   
   <div class="posttitle">
      <a href="/trojan-puzzle-attack-trains-ai-assistants-into-suggesting-malicious-code/">Trojan Puzzle attack trains AI assistants into suggesting malicious code</a>


   </div>
   
   <div class="posttitle">
      <a href="/dissecting-distribution-inference/">Dissecting Distribution Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/cray-distinguished-speaker-on-leaky-models-and-unintended-inferences/">Cray Distinguished Speaker: On Leaky Models and Unintended Inferences</a>


   </div>
   
   <div class="posttitle">
      <a href="/attribute-inference-attacks-are-really-imputation/">Attribute Inference attacks are really Imputation</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-jayaraman/">Congratulations, Dr. Jayaraman!</a>


   </div>
   
   <div class="posttitle">
      <a href="/balancing-tradeoffs-between-fickleness-and-obstinacy-in-nlp-models/">Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</a>


   </div>
   
   <div class="posttitle">
      <a href="/best-submission-award-at-visxai-2022/">Best Submission Award at VISxAI 2022</a>


   </div>
   
   <div class="posttitle">
      <a href="/visualizing-poisoning/">Visualizing Poisoning</a>


   </div>
   
   <div class="posttitle">
      <a href="/congratulations-dr.-zhang/">Congratulations, Dr. Zhang!</a>


   </div>
   
   <div class="posttitle">
      <a href="/biml-what-machine-learnt-models-reveal/">BIML: What Machine Learnt Models Reveal</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-2022-understanding-intrinsic-robustness-using-label-uncertainty/">ICLR 2022: Understanding Intrinsic Robustness Using Label Uncertainty</a>


   </div>
   
   <div class="posttitle">
      <a href="/microsoft-research-summit-surprising-and-unsurprising-inference-risks-in-machine-learning/">Microsoft Research Summit: Surprising (and unsurprising) Inference Risks in Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/uva-news-article/">UVA News Article</a>


   </div>
   
   <div class="posttitle">
      <a href="/model-targeted-poisoning-attacks-with-provable-convergence/">Model-Targeted Poisoning Attacks with Provable Convergence</a>


   </div>
   
   <div class="posttitle">
      <a href="/on-the-risks-of-distribution-inference/">On the Risks of Distribution Inference</a>


   </div>
   
   <div class="posttitle">
      <a href="/chinese-translation-of-mpc-book/">Chinese Translation of MPC Book</a>


   </div>
   
   <div class="posttitle">
      <a href="/iclr-dpml-2021-inference-risks-for-machine-learning/">ICLR DPML 2021: Inference Risks for Machine Learning</a>


   </div>
   
   <div class="posttitle">
      <a href="/how-to-hide-a-backdoor/">How to Hide a Backdoor</a>


   </div>
   
   <div class="posttitle">
      <a href="/codaspy-2021-keynote-when-models-learn-too-much/">Codaspy 2021 Keynote: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/crysp-talk-when-models-learn-too-much/">CrySP Talk: When Models Learn Too Much</a>


   </div>
   
   <div class="posttitle">
      <a href="/improved-estimation-of-concentration-iclr-2021/">Improved Estimation of Concentration (ICLR 2021)</a>


   </div>
   
   <div class="posttitle">
      <a href="/virginia-consumer-data-protection-act/">Virginia Consumer Data Protection Act</a>


   </div>
   
<p></p>
   <div class="posttitle"><a href="/post/">Older Posts</a></div>
  <div class="posttitle"><a href="/tags">Posts by Tag</a></div>
  <div class="posttitle"><a href="/categories/">Posts by Category</a></div>
  <div class="posttitle"><a href="2017.html">Old Blog</a></div>
<p></p>
<p>
<a href="/awards/"><b>Awards</b></a>
</p>

   <p>
Director: <a href="//www.cs.virginia.edu/evans">David Evans</a>
   </p>

<p>

  </p>

<p><br></br></p>

   <p>
     <center>
       <img src="/images/uva_primary_rgb_white.png" width="80%">
       </center>
</p>

    </div>
</div>

   </div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-8 medium-4">
      
      <a href="/"><img src="/images/srg-logo-scaled.png" width=100 height=100 alt="SRG Logo" align="left"> <b>Security Research Group</b></a><br>
      <a href="//www.cs.virginia.edu/">University of Virginia</a><br>
    </div>
    <div class="column small-6 medium-3">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="//uvasrg.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
