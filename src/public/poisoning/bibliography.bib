@misc{suya2021modeltargeted,
      title={Model-Targeted Poisoning Attacks with Provable Convergence}, 
      author={Fnu Suya and Saeed Mahloujifar and Anshuman Suri and David Evans and Yuan Tian},
      year={2021},
      eprint={2006.16469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{su2017pixel,
      title={One pixel attack for fooling deep neural networks}, 
      author={Jiawei Su and Danilo Vasconcellos Vargas and Sakurai Kouichi},
      year={2017},
      eprint={1710.08864},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{biggio2012poisoning,
      title={Poisoning Attacks against Support Vector Machines}, 
      author={Battista Biggio and Blaine Nelson and Pavel Laskov},
      year={2012},
      eprint={1206.6389},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jagielski2021subpopulation,
      title={Subpopulation Data Poisoning Attacks}, 
      author={Matthew Jagielski and Giorgio Severi and Niklas Pousette Harger and Alina Oprea},
      year={2021},
      eprint={2006.14026},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@misc{szegedy2013intriguing,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2013},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{10.1145/2046684.2046692,
author = {Huang, Ling and Joseph, Anthony D. and Nelson, Blaine and Rubinstein, Benjamin I.P. and Tygar, J. D.},
title = {Adversarial Machine Learning},
year = {2011},
isbn = {9781450310031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046684.2046692},
doi = {10.1145/2046684.2046692},
abstract = {In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.},
booktitle = {Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence},
pages = {43–58},
numpages = {16},
keywords = {security metrics, statistical learning, adversarial learning, spam filters, game theory, computer security, machine learning, intrusion detection},
location = {Chicago, Illinois, USA},
series = {AISec '11}
}

@inproceedings{10.5555/3007337.3007488,
author = {Xiao, Han and Xiao, Huang and Eckert, Claudia},
title = {Adversarial Label Flips Attack on Support Vector Machines},
year = {2012},
isbn = {9781614990970},
publisher = {IOS Press},
address = {NLD},
abstract = {To develop a robust classification algorithm in the adversarial setting, it is important to understand the adversary's strategy. We address the problem of label flips attack where an adversary contaminates the training set through flipping labels. By analyzing the objective of the adversary, we formulate an optimization framework for finding the label flips that maximize the classification error. An algorithm for attacking support vector machines is derived. Experiments demonstrate that the accuracy of classifiers is significantly degraded under the attack.},
booktitle = {Proceedings of the 20th European Conference on Artificial Intelligence},
pages = {870–875},
numpages = {6},
location = {Montpellier, France},
series = {ECAI'12}
}

@inproceedings{10.5555/2886521.2886721,
author = {Mei, Shike and Zhu, Xiaojin},
title = {Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understanding training-set attacks is important as more intelligent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This paper identifies the optimal training-set attack on a broad family of machine learners. First we show that optimal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gradient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support Vector Machines, logistic regression, and linear regression with extensive experiments. Finally, we discuss potential defenses against such attacks.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2871–2877},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@misc{steinhardt2017certified,
      title={Certified Defenses for Data Poisoning Attacks}, 
      author={Jacob Steinhardt and Pang Wei Koh and Percy Liang},
      year={2017},
      eprint={1706.03691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{koh2018stronger,
      title={Stronger Data Poisoning Attacks Break Data Sanitization Defenses}, 
      author={Pang Wei Koh and Jacob Steinhardt and Percy Liang},
      year={2018},
      eprint={1811.00741},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{shafahi2018poison,
      title={Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks}, 
      author={Ali Shafahi and W. Ronny Huang and Mahyar Najibi and Octavian Suciu and Christoph Studer and Tudor Dumitras and Tom Goldstein},
      year={2018},
      eprint={1804.00792},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhu2019transferable,
      title={Transferable Clean-Label Poisoning Attacks on Deep Neural Nets}, 
      author={Chen Zhu and W. Ronny Huang and Ali Shafahi and Hengduo Li and Gavin Taylor and Christoph Studer and Tom Goldstein},
      year={2019},
      eprint={1905.05897},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{koh2017understanding,
      title={Understanding Black-box Predictions via Influence Functions}, 
      author={Pang Wei Koh and Percy Liang},
      year={2017},
      eprint={1703.04730},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{geiping2020witches,
      title={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching}, 
      author={Jonas Geiping and Liam Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},
      year={2020},
      eprint={2009.02276},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{huang2020metapoison,
      title={MetaPoison: Practical General-purpose Clean-label Data Poisoning}, 
      author={W. Ronny Huang and Jonas Geiping and Liam Fowl and Gavin Taylor and Tom Goldstein},
      year={2020},
      eprint={2004.00225},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{decrema2019progress,
   title={Are we really making much progress? A worrying analysis of recent neural recommendation approaches},
   url={http://dx.doi.org/10.1145/3298689.3347058},
   DOI={10.1145/3298689.3347058},
   journal={Proceedings of the 13th ACM Conference on Recommender Systems},
   publisher={ACM},
   author={Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
   year={2019},
   month={Sep}
}

@misc{tramer2021differentially,
      title={Differentially Private Learning Needs Better Features (or Much More Data)}, 
      author={Florian Tramèr and Dan Boneh},
      year={2021},
      eprint={2011.11660},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.5555/1387709.1387716,
author = {Nelson, Blaine and Barreno, Marco and Chi, Fuching Jack and Joseph, Anthony D. and Rubinstein, Benjamin I. P. and Saini, Udam and Sutton, Charles and Tygar, J. D. and Xia, Kai},
title = {Exploiting Machine Learning to Subvert Your Spam Filter},
year = {2008},
publisher = {USENIX Association},
address = {USA},
abstract = {Using statistical machine learning for making security decisions introduces new vulnerabilities in large scale systems. This paper shows how an adversary can exploit statistical machine learning, as used in the SpamBayes spam filter, to render it useless--even if the adversary's access is limited to only 1% of the training messages. We further demonstrate a new class of focused attacks that successfully prevent victims from receiving specific email messages. Finally, we introduce two new types of defenses against these attacks.},
booktitle = {Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats},
articleno = {7},
numpages = {9},
location = {San Francisco, California},
series = {LEET'08}
}