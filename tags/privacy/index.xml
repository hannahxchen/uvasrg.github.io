<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>privacy on Security Research Group</title>
    <link>//uvasrg.github.io/tags/privacy/</link>
    <description>Recent content in privacy on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 28 Sep 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="//uvasrg.github.io/tags/privacy/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Computer science professor David Evans and his team conduct experiments to understand security and privacy risks associated with machine learning</title>
      <link>//uvasrg.github.io/computer-science-professor-david-evans-and-his-team-conduct-experiments-to-understand-security-and-privacy-risks-associated-with-machine-learning/</link>
      <pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/computer-science-professor-david-evans-and-his-team-conduct-experiments-to-understand-security-and-privacy-risks-associated-with-machine-learning/</guid>
      <description>UVA News has an article by Audra Book on our research on security and privacy of machine learning (with some very nice quotes from several students in the group!): Computer science professor David Evans and his team conduct experiments to understand security and privacy risks associated with machine learning, 8 September 2021.
 David Evans, professor of computer science in the University of Virginia School of Engineering and Applied Science, is leading research to understand how machine learning models can be compromised.</description>
    </item>
    
    <item>
      <title>ICLR DPML 2021: Inference Risks for Machine Learning</title>
      <link>//uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/iclr-dpml-2021-inference-risks-for-machine-learning/</guid>
      <description>I gave an invited talk at the Distributed and Private Machine Learning (DPML) workshop at ICLR 2021 on Inference Risks for Machine Learning.
  The talk mostly covers work by Bargav Jayaraman on evaluating privacy in machine learning and connecting attribute inference and imputation, and recent work by Anshuman Suri on property inference.</description>
    </item>
    
    <item>
      <title>Codaspy 2021 Keynote: When Models Learn Too Much</title>
      <link>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</link>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/codaspy-2021-keynote-when-models-learn-too-much/</guid>
      <description>Here are the slides for my talk at the 11th ACM Conference on Data and Application Security and Privacy:
 When Models Learn Too Much [PDF]   The talk includes Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy in machine learning, as well as more recent work by Anshuman Suri on property inference attacks, and Bargav on attribute inference and imputation:
 Merlin, Morgan, and the Importance of Thresholds and Priors Evaluating Differentially Private Machine Learning in Practice   “When models learn too much.</description>
    </item>
    
    <item>
      <title>Virginia Consumer Data Protection Act</title>
      <link>//uvasrg.github.io/virginia-consumer-data-protection-act/</link>
      <pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/virginia-consumer-data-protection-act/</guid>
      <description>Josephine Lamp presented on the new data privacy law that is pending in Virginia (it still needs a few steps including expected signing by governor, but likely to go into effect Jan 1, 2023): Slides (PDF)
This article provides a summary of the law: Virginia Passes Consumer Privacy Law; Other States May Follow, National Law Review, 17 February 2021.
The law itself is here: SB 1392: Consumer Data Protection Act</description>
    </item>
    
    <item>
      <title>Microsoft Security Data Science Colloquium: Inference Privacy in Theory and Practice</title>
      <link>//uvasrg.github.io/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/microsoft-security-data-science-colloquium-inference-privacy-in-theory-and-practice/</guid>
      <description>Here are the slides for my talk at the Microsoft Security Data Science Colloquium:
When Models Learn Too Much: Inference Privacy in Theory and Practice [PDF]
   The talk is mostly about Bargav Jayaraman&amp;rsquo;s work (with Katherine Knipmeyer, Lingxiao Wang, and Quanquan Gu) on evaluating privacy:
 Merlin, Morgan, and the Importance of Thresholds and Priors Evaluating Differentially Private Machine Learning in Practice  </description>
    </item>
    
    <item>
      <title>Merlin, Morgan, and the Importance of Thresholds and Priors</title>
      <link>//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/merlin-morgan-and-the-importance-of-thresholds-and-priors/</guid>
      <description>Post by Katherine Knipmeyer
Machine learning poses a substantial risk that adversaries will be able to discover information that the model does not intend to reveal. One set of methods by which consumers can learn this sensitive information, known broadly as membership inference attacks, predicts whether or not a query record belongs to the training set. A basic membership inference attack involves an attacker with a given record and black-box access to a model who tries to determine whether said record was a member of the model’s training set.</description>
    </item>
    
    <item>
      <title>White House Visit</title>
      <link>//uvasrg.github.io/white-house-visit/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/white-house-visit/</guid>
      <description>I had a chance to visit the White House for a Roundtable on Accelerating Responsible Sharing of Federal Data. The meeting was held under &amp;ldquo;Chatham House Rules&amp;rdquo;, so I won&amp;rsquo;t mention the other participants here.
   The meeting was held in the Roosevelt Room of the White House. We entered through the visitor&amp;rsquo;s side entrance. After a security gate (where you put your phone in a lockbox, so no pictures inside) with a TV blaring Fox News, there is a pleasant lobby for waiting, and then an entrance right into the Roosevelt Room.</description>
    </item>
    
    <item>
      <title>Research Symposium Posters</title>
      <link>//uvasrg.github.io/research-symposium-posters/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/research-symposium-posters/</guid>
      <description>Five students from our group presented posters at the department&amp;rsquo;s Fall Research Symposium:
 
Anshuman Suri&#39;s Overview Talk   Bargav Jayaraman, Evaluating Differentially Private Machine Learning In Practice [Poster]
[Paper (USENIX Security 2019)]  

 Hannah Chen [Poster]  

 Xiao Zhang [Poster]
[Paper (NeurIPS 2019)]  

 Mainudding Jonas [Poster]  

 Fnu Suya [Poster]
[Paper (USENIX Security 2020)]  </description>
    </item>
    
    <item>
      <title>FOSAD Trustworthy Machine Learning Mini-Course</title>
      <link>//uvasrg.github.io/fosad2019/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/fosad2019/</guid>
      <description>I taught a mini-course on Trustworthy Machine Learning at the 19th International School on Foundations of Security Analysis and Design in Bertinoro, Italy.
 Slides from my three (two-hour) lectures are posted below, along with some links to relevant papers and resources.
Class 1: Introduction/Attacks    The PDF malware evasion attack is described in this paper:
 Weilin Xu, Yanjun Qi, and David Evans. Automatically Evading Classifiers: A Case Study on PDF Malware Classifiers.</description>
    </item>
    
    <item>
      <title>Evaluating Differentially Private Machine Learning in Practice</title>
      <link>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</link>
      <pubDate>Tue, 27 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/evaluating-differentially-private-machine-learning-in-practice/</guid>
      <description>(Cross-post by Bargav Jayaraman)
With the recent advances in composition of differential private mechanisms, the research community has been able to achieve meaningful deep learning with privacy budgets in single digits. Rènyi differential privacy (RDP) is one mechanism that provides tighter composition which is widely used because of its implementation in TensorFlow Privacy (recently, Gaussian differential privacy (GDP) has shown a tighter analysis for low privacy budgets, but it was not yet available when we did this work).</description>
    </item>
    
    <item>
      <title>USENIX Security Symposium 2019</title>
      <link>//uvasrg.github.io/usenix-security-symposium-2019/</link>
      <pubDate>Mon, 26 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/usenix-security-symposium-2019/</guid>
      <description>Bargav Jayaraman presented our paper on Evaluating Differentially Private Machine Learning in Practice at the 28th USENIX Security Symposium in Santa Clara, California.
       Summary by Lea Kissner:
 Hey it&amp;#39;s the results! pic.twitter.com/ru1FbkESho
&amp;mdash; Lea Kissner (@LeaKissner) August 17, 2019   Also, great to see several UVA folks at the conference including:
 Sam Havron (BSCS 2017, now a PhD student at Cornell) presented a paper on the work he and his colleagues have done on computer security for victims of intimate partner violence.</description>
    </item>
    
    <item>
      <title>Brink Essay: AI Systems Are Complex and Fragile. Here Are Four Key Risks to Understand.</title>
      <link>//uvasrg.github.io/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/brink-essay-ai-systems-are-complex-and-fragile.-here-are-four-key-risks-to-understand./</guid>
      <description>Brink News (a publication of The Atlantic) published my essay on the risks of deploying AI systems.
   Artificial intelligence technologies have the potential to transform society in positive and powerful ways. Recent studies have shown computing systems that can outperform humans at numerous once-challenging tasks, ranging from performing medical diagnoses and reviewing legal contracts to playing Go and recognizing human emotions. 
Despite these successes, AI systems are fundamentally fragile — and the ways they can fail are poorly understood.</description>
    </item>
    
    <item>
      <title>Google Federated Privacy 2019: The Dragon in the Room</title>
      <link>//uvasrg.github.io/google-federated-privacy-2019-the-dragon-in-the-room/</link>
      <pubDate>Sat, 22 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/google-federated-privacy-2019-the-dragon-in-the-room/</guid>
      <description>I&amp;rsquo;m back from a very interesting Workshop on Federated Learning and Analytics that was organized by Peter Kairouz and Brendan McMahan from Google&amp;rsquo;s federated learning team and was held at Google Seattle.
For the first part of my talk, I covered Bargav&amp;rsquo;s work on evaluating differentially private machine learning, but I reserved the last few minutes of my talk to address the cognitive dissonance I felt being at a Google meeting on privacy.</description>
    </item>
    
    <item>
      <title>Violations of Children’s Privacy Laws</title>
      <link>//uvasrg.github.io/violations-of-childrens-privacy-laws/</link>
      <pubDate>Sun, 16 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/violations-of-childrens-privacy-laws/</guid>
      <description>The New York Times has an article, How Game Apps That Captivate Kids Have Been Collecting Their Data about a lawsuit the state of New Mexico is bringing against app markets (including Google) that allow apps presented as being for children in the Play store to violate COPPA rules and mislead users into tracking children. The lawsuit stems from a study led by Serge Egleman’s group at UC Berkeley that analyzed COPPA violations in children’s apps.</description>
    </item>
    
    <item>
      <title>USENIX Security 2018</title>
      <link>//uvasrg.github.io/usenix-security-2018/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>//uvasrg.github.io/usenix-security-2018/</guid>
      <description>Three SRG posters were presented at USENIX Security Symposium 2018 in Baltimore, Maryland:
 Nathaniel Grevatt (GDPR-Compliant Data Processing: Improving Pseudonymization with Multi-Party Computation) Matthew Wallace and Parvesh Samayamanthula (Deceiving Privacy Policy Classifiers with Adversarial Examples) Guy Verrier (How is GDPR Affecting Privacy Policies?, joint with Haonan Chen and Yuan Tian)  
         There were also a surprising number of appearances by an unidentified unicorn:</description>
    </item>
    
  </channel>
</rss>