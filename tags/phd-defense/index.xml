<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PhD defense on Security Research Group</title>
    <link>//uvasrg.github.io/tags/phd-defense/</link>
    <description>Recent content in PhD defense on Security Research Group</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Privacy and Security Research at the University of Virginia</copyright>
    <lastBuildDate>Tue, 11 Jul 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="//uvasrg.github.io/tags/phd-defense/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Congratulations, Dr. Suya!</title>
      <link>//uvasrg.github.io/congratulations-dr.-suya/</link>
      <pubDate>Tue, 11 Jul 2023 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-suya/</guid>
      <description>Congratulations to Fnu Suya for successfully defending his PhD thesis!&#xA;Suya will join the Unversity of Maryland as a MC2 Postdoctoral Fellow at the Maryland Cybersecurity Center this fall.&#xA;On the Limits of Data Poisoning Attacks Current machine learning models require large amounts of labeled training data, which are often collected from untrusted sources. Models trained on these potentially manipulated data points are prone to data poisoning attacks. My research aims to gain a deeper understanding on the limits of two types of data poisoning attacks: indiscriminate poisoning attacks, where the attacker aims to increase the test error on the entire dataset; and subpopulation poisoning attacks, where the attacker aims to increase the test error on a defined subset of the distribution.</description>
    </item>
    <item>
      <title>Congratulations, Dr. Jayaraman!</title>
      <link>//uvasrg.github.io/congratulations-dr.-jayaraman/</link>
      <pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-jayaraman/</guid>
      <description>Congratulations to Bargav Jayaraman for successfully defending his PhD thesis!&#xA;Dr. Jayaraman and his PhD committee: Mohammad&amp;nbsp;Mahmoody, Quanquan&amp;nbsp;Gu (UCLA Department of Computer Science, on screen), Yanjun&amp;nbsp;Qi (Committee Chair, on screen), Denis&amp;nbsp;Nekipelov (Department of Economics, on screen), and David Evans Bargav will join the Meta AI Lab in Menlo Park, CA as a post-doctoral researcher.&#xA;Analyzing the Leaky Cauldron: Inference Attacks on Machine Learning Machine learning models have been shown to leak sensitive information about their training data.</description>
    </item>
    <item>
      <title>Congratulations, Dr. Zhang!</title>
      <link>//uvasrg.github.io/congratulations-dr.-zhang/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-zhang/</guid>
      <description>Congratulations to Xiao Zhang for successfully defending his PhD thesis!&#xA;Dr. Zhang and his PhD committee: Somesh Jha (University of Wisconsin), David Evans, Tom Fletcher; Tianxi&amp;nbsp;Li&amp;nbsp;(UVA&amp;nbsp;Statistics), David&amp;nbsp;Wu&amp;nbsp;(UT&amp;nbsp;Austin), Mohammad&amp;nbsp;Mahmoody; Xiao&amp;nbsp;Zhang. Xiao will join the CISPA Helmholtz Center for Information Security in Saarbrücken, Germany this fall as a tenure-track faculty member.&#xA;From Characterizing Intrinsic Robustness to Adversarially Robust Machine Learning The prevalence of adversarial examples raises questions about the reliability of machine learning systems, especially for their deployment in critical applications.</description>
    </item>
    <item>
      <title>Congratulations Dr. Xu!</title>
      <link>//uvasrg.github.io/congratulations-dr.-xu/</link>
      <pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate>
      <guid>//uvasrg.github.io/congratulations-dr.-xu/</guid>
      <description>Congratulations to Weilin Xu for successfully defending his PhD Thesis!&#xA;Weilin&#39;s Committee: Homa Alemzadeh, Yanjun Qi, Patrick McDaniel (on screen), David Evans, Vicente Ordóñez Román Improving Robustness of Machine Learning Models using Domain Knowledge Although machine learning techniques have achieved great success in many areas, such as computer vision, natural language processing, and computer security, recent studies have shown that they are not robust under attack. A motivated adversary is often able to craft input samples that force a machine learning model to produce incorrect predictions, even if the target model achieves high accuracy on normal test inputs.</description>
    </item>
  </channel>
</rss>
